{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community\n",
        "!pip install langchain_google_vertexai\n",
        "!pip install PyPdf"
      ],
      "metadata": {
        "id": "5D-tn-imrkvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "23Bzcpe_gWFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DCBTcspp0X-"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import VertexAI, ChatVertexAI, VertexAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID=\"your-gcp-project-id\"\n",
        "REGION=\"region of your vertex ai enabled api\""
      ],
      "metadata": {
        "id": "YoF8jKdKqGVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "XYpg3616qPqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai"
      ],
      "metadata": {
        "id": "QHsE1Cf9qWCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "Kifa4Y4UqYwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# import zipfile\n",
        "# import requests\n",
        "\n",
        "# logging.basicCOnfig(level=logging.INFO)\n",
        "\n",
        "# data_url="
      ],
      "metadata": {
        "id": "AnKIUPfrqgEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./files/doc3.pdf\")\n",
        "docs=loader.load()\n",
        "tables=[]\n",
        "texts = [d.page_content for d in docs]"
      ],
      "metadata": {
        "id": "Q8wbJ0S2q5dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "id": "VtFhOsXjrVTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "CsXqK1tQs80f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "  \"\"\"\n",
        "\n",
        "  Summarize text elements\n",
        "  texts: List of str\n",
        "  tables: List of str\n",
        "  summarize_texts: Bool to summarize texts\n",
        "  \"\"\"\n",
        "\n",
        "  # Prompt\n",
        "  prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "  These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "  Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate.from_template(prompt_text)\n",
        "  empty_response = RunnableLambda(\n",
        "      lambda x: AIMessage(content=\"Error processing document\")\n",
        "  )\n",
        "  # Text summary chain\n",
        "  model = VertexAI(\n",
        "      temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n",
        "  ).with_fallbacks([empty_response])\n",
        "  summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "  # Initialize empty summaries\n",
        "  text_summaries = []\n",
        "  table_summaries = []\n",
        "\n",
        "  # Apply to text if texts are provided and summarization is requested\n",
        "  if texts and summarize_texts:\n",
        "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "  elif texts:\n",
        "    text_summaries = texts\n",
        "\n",
        "  # Apply to tables if tables are provided\n",
        "  if tables:\n",
        "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "  return text_summaries, table_summaries\n",
        "\n",
        "# Get text, table summaries\n",
        "text_summaries, table_summaries = generate_text_summaries(\n",
        "    texts, tables, summarize_texts=True\n",
        ")"
      ],
      "metadata": {
        "id": "kxgHDkgDtRal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_summaries)"
      ],
      "metadata": {
        "id": "9HQHBMb-xR10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497e716f-d8f9-465b-c02f-4ac11f1fe5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries[44]"
      ],
      "metadata": {
        "id": "o6mMWWGHfqH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "def encode_image(image_path):\n",
        "  \"\"\"Getting the base64 string\"\"\"\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "  \"\"\"Make image summary\"\"\"\n",
        "  model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
        "\n",
        "  msg = model(\n",
        "      [\n",
        "          HumanMessage(\n",
        "              content=[\n",
        "                  {\"type\": \"text\", \"text\": prompt},\n",
        "                  {\n",
        "                      \"type\": \"image_url\",\n",
        "                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                  },\n",
        "              ]\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "  return msg.content\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "  \"\"\"\n",
        "  Generate summaries and base64 encoded strings for images\n",
        "  path: Path to list of .jpg files extracted by Unstructured\n",
        "  \"\"\"\n",
        "\n",
        "  # Store base64 encoded images\n",
        "  img_base64_list=[]\n",
        "\n",
        "  # Store image summaries\n",
        "  image_summaries = []\n",
        "\n",
        "  # Prompt\n",
        "  prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "  These summaries will be embedded and used to retrieve the raw image. \\\n",
        "  Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "  # Apply to images\n",
        "  for img_file in sorted(os.listdir(path)):\n",
        "    if img_file.endswith(\".jpg\"):\n",
        "      img_path = os.path.join(path, img_file)\n",
        "      base64_image = encode_image(img_path)\n",
        "      img_base64_list.append(base64_image)\n",
        "      image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "  return img_base64_list, image_summaries\n",
        "\n",
        "\n",
        "# Image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(\"files\")"
      ],
      "metadata": {
        "id": "k3h7FqtM1JRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_base64_list)"
      ],
      "metadata": {
        "id": "dhHo_Yl44kON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def8d9f5-991b-4777-df3d-8b575ce277c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_summaries)"
      ],
      "metadata": {
        "id": "9X3CugH34j8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7338178c-6ef0-43d0-8d0d-75d74a5cec7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries[0]"
      ],
      "metadata": {
        "id": "n-S1tRHm4jzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def create_multi_vector_retriever(\n",
        "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "):\n",
        "    \"\"\"\n",
        "    Create retriever that indexes summaries but returns raw images or texts\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the storage layer\n",
        "    store = InMemoryStore()\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    # Create the multi-vector retriever\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore = vectorstore,\n",
        "        docstore = store,\n",
        "        id_key = id_key,\n",
        "    )\n",
        "\n",
        "    # Helper function to add documents to the vectorstore and docstore\n",
        "    def add_documents(retriever, doc_summaries, doc_contents):\n",
        "      doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "      summary_docs = [\n",
        "          Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "          for i,s, in enumerate(doc_summaries)\n",
        "      ]\n",
        "      retriever.vectorstore.add_documents(summary_docs)\n",
        "      retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "    # Add texts, tables, and images\n",
        "    # Check that text_summaries is not empty before adding\n",
        "    if text_summaries:\n",
        "      add_documents(retriever, text_summaries, texts)\n",
        "    # CHeck that table_summaries is not empty before adding\n",
        "    if table_summaries:\n",
        "      add_documents(retriever, table_summaries, tables)\n",
        "    # Check that image_summaries is not empty before adding\n",
        "    if image_summaries:\n",
        "      add_documents(retriever, image_summaries, images)\n",
        "\n",
        "    return retriever\n",
        "\n",
        "# The vectorstore to use to index the summaries\n",
        "vectorstore = Chroma(\n",
        "    collection_name = \"mm_rag_cj_blog\",\n",
        "    embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n",
        ")\n",
        "\n",
        "# Create Retriever\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "    vectorstore,\n",
        "    text_summaries,\n",
        "    texts,\n",
        "    table_summaries,\n",
        "    tables,\n",
        "    image_summaries,\n",
        "    img_base64_list,\n",
        ")\n"
      ],
      "metadata": {
        "id": "BAyjs9404zM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from PIL import Image\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "  \"\"\"Display base64 encoded string as image\"\"\"\n",
        "  # Create an HTML img tag with the base64 string as source\n",
        "  image_html = f'<img src=\"data\"image/jpeg;base64,{img_base64}\" />'\n",
        "  # Display the image by rendering the HTML\n",
        "  display(HTML(image_html))\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "  \"\"\"CHceck if the string looks like base64\"\"\"\n",
        "  return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "def is_image_data(b64data):\n",
        "  \"\"\"\n",
        "  CHeck if the base64 data is an image by looking at the start of the data\n",
        "  \"\"\"\n",
        "  image_signatures = {\n",
        "      b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "      b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "      b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "      b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "  }\n",
        "  try:\n",
        "    header = base64.b64decode(b64data)[:8] # Decode and get the first 8 bytes\n",
        "    for sig, format in image_signatures.items():\n",
        "      if header.startswith(sig):\n",
        "        return True\n",
        "    return False\n",
        "  except Exception:\n",
        "    return False\n",
        "\n",
        "def resize_base64_image(base64_string, size=(128,128)):\n",
        "  \"\"\"\n",
        "  Resize an image encoded as a Base64 string\n",
        "  \"\"\"\n",
        "  # Decode the base64 string\n",
        "  img_data = base64.b64decode(base64_string)\n",
        "  img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "  # Resize the image\n",
        "  resized_img = img.resize(size, Image.LANCZOS)\n",
        "\n",
        "  # Save the resized image to a bytes buffer\n",
        "  buffered = io.BytesIO()\n",
        "  resized_img.save(buffered, format=img.format)\n",
        "\n",
        "  # Encode the resized image to Base64\n",
        "  return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "  \"\"\"\n",
        "  Split base64-encoded images and texts\n",
        "  \"\"\"\n",
        "  b64_images=[]\n",
        "  texts = []\n",
        "  for doc in docs:\n",
        "    # CHeck if the document is of tpye Document and extract page_content if so\n",
        "    if isinstance(doc, Document):\n",
        "      doc = doc.page_content\n",
        "    if looks_like_base64(doc) and is_image_data(doc):\n",
        "      doc = resize_base64_image(doc, size=(1300,600))\n",
        "      b64_images.append(doc)\n",
        "    else:\n",
        "      texts.append(doc)\n",
        "  if len(b64_images) > 0:\n",
        "    return {\"images\": b64_images[:1], \"texts\": []}\n",
        "  return {\"images\":b64_images, \"texts\": texts}\n",
        "\n",
        "\n",
        "def img_prompt_func(data_dict):\n",
        "  \"\"\"\n",
        "  Join the context into a single string\n",
        "  \"\"\"\n",
        "\n",
        "  formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "  messages = []\n",
        "\n",
        "  # Adding the text for analysis\n",
        "  text_message = {\n",
        "      \"type\": \"text\",\n",
        "      \"text\": (\n",
        "          \"You are financial analyst tasking with providing investment advice.\\n\"\n",
        "          \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
        "          \"Use this information to provide investment advice related to the user question. \\n\"\n",
        "          f\"Use-provided question: {data_dict['question']}\\n\\n\"\n",
        "          \"Text and/or tables: \\n\"\n",
        "          f\"{formatted_texts}\"\n",
        "      ),\n",
        "  }\n",
        "  messages.append(text_message)\n",
        "  # Adding image(s) to the messages if present\n",
        "  if data_dict[\"context\"][\"images\"]:\n",
        "    for image in data_dict[\"context\"][\"images\"]:\n",
        "      image_message = {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "      }\n",
        "      messages.append(image_message)\n",
        "  return [HumanMessage(content=messages)]\n",
        "\n",
        "\n",
        "def multi_modal_rag_chain(retriever):\n",
        "  \"\"\"\n",
        "  Multi-modal RAG chain\n",
        "  \"\"\"\n",
        "\n",
        "  # Multi-modal LLM\n",
        "  model = ChatVertexAI(\n",
        "      temperature = 0, model_name = \"gemini-pro-vision\", max_output_tokens=1024\n",
        "  )\n",
        "\n",
        "  # RAG Pipeline\n",
        "  chain = (\n",
        "      {\n",
        "          \"context\": retriever | RunnableLambda(split_image_text_types),\n",
        "          \"question\": RunnablePassthrough(),\n",
        "      }\n",
        "      | RunnableLambda(img_prompt_func)\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return chain\n",
        "\n",
        "\n",
        "# Create RAG chain\n",
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
      ],
      "metadata": {
        "id": "BVYPNd7N_Eso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is nifty value for March-2023?\"\n",
        "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=1)"
      ],
      "metadata": {
        "id": "smrNltyWkd7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjo90PwFlwzZ",
        "outputId": "75364af1-dcd4-47f6-dd62-90a4f76f864f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(docs[3])"
      ],
      "metadata": {
        "id": "UwwJF6ool2Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain_multimodal_rag.invoke(query)"
      ],
      "metadata": {
        "id": "Qm_7BO6hmHvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown as md"
      ],
      "metadata": {
        "id": "a55ZjqnrmPhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md(result)"
      ],
      "metadata": {
        "id": "AmlFa9RumU81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain_multimodal_rag.invoke(\"what is the EBITDA for the titan company?\")\n",
        "md(result)"
      ],
      "metadata": {
        "id": "DXxQ0-Ukmb5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = chain_multimodal_rag.invoke(\"what is the Food delivery - GOV (INR bn) for zomato for Q1FY24?\")\n",
        "# result = chain_multimodal_rag.invoke(\"what are all Adjusted EBITDA reported for zomato in FY24?\")\n",
        "# result = chain_multimodal_rag.invoke(\"what is the trend of Food delivery - contribution margin as a % of GOV of zomato?\")\n",
        "# result = chain_multimodal_rag.invoke(\"what is the bloomberg code of InterGlobe Aviation?\")\n",
        "result = chain_multimodal_rag.invoke(\"indigo in was not flyng to what places before which it is flyng now?\")\n",
        "md(result)"
      ],
      "metadata": {
        "id": "xsQ6BGT0njSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9k3bk057XBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}