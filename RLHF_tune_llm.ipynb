{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1073ae28",
   "metadata": {},
   "source": [
    "# Lesson 3: Tune an LLM with RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8bc26",
   "metadata": {},
   "source": [
    "#### Project environment setup\n",
    "\n",
    "The RLHF training process has been implemented in a machine learning pipeline as part of the (Google Cloud Pipeline Components) library. This can be run on any platform that supports KubeFlow Pipelines (an open source framework), and can also run on Google Cloud's Vertex AI Pipelines.\n",
    "\n",
    "To run it locally, install the following:\n",
    "\n",
    "```Python\n",
    "!pip3 install google-cloud-pipeline-components\n",
    "!pip3 install kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c7818",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8289be-0f03-4f97-aaf3-b4e80330bce9",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Import (RLFH is currently in preview)\n",
    "from google_cloud_pipeline_components.preview.llm \\\n",
    "import rlhf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ac4718-782d-4fe7-a39f-51fe5b423210",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Import from KubeFlow pipelines\n",
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef016774-5400-45be-9682-4b0b0daa9095",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Define a path to the yaml file\n",
    "RLHF_PIPELINE_PKG_PATH = \"rlhf_pipeline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abca02b-c43b-4301-8be0-a34949eb38b0",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Execute the compile function\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=rlhf_pipeline,\n",
    "    package_path=RLHF_PIPELINE_PKG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3904e2b-593c-4c95-b0c8-f7334a1fa728",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\r\n",
      "# Name: rlhf-train-template\r\n",
      "# Description: Performs reinforcement learning from human feedback.\r\n",
      "# Inputs:\r\n",
      "#    deploy_model: bool [Default: True]\r\n",
      "#    eval_dataset: str\r\n",
      "#    instruction: str\r\n",
      "#    kl_coeff: float [Default: 0.1]\r\n",
      "#    large_model_reference: str\r\n",
      "#    location: str [Default: '{{$.pipeline_google_cloud_location}}']\r\n"
     ]
    }
   ],
   "source": [
    "# Print the first lines of the YAML file\n",
    "!head rlhf_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951da6e9",
   "metadata": {},
   "source": [
    "**Note**: to print the whole YAML file, use the following:\n",
    "```Python\n",
    "!cat rlhf_pipeline.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654f674a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\r\n",
      "# Name: rlhf-train-template\r\n",
      "# Description: Performs reinforcement learning from human feedback.\r\n",
      "# Inputs:\r\n",
      "#    deploy_model: bool [Default: True]\r\n",
      "#    eval_dataset: str\r\n",
      "#    instruction: str\r\n",
      "#    kl_coeff: float [Default: 0.1]\r\n",
      "#    large_model_reference: str\r\n",
      "#    location: str [Default: '{{$.pipeline_google_cloud_location}}']\r\n",
      "#    model_display_name: str\r\n",
      "#    preference_dataset: str\r\n",
      "#    project: str [Default: '{{$.pipeline_google_cloud_project_id}}']\r\n",
      "#    prompt_dataset: str\r\n",
      "#    prompt_sequence_length: int [Default: 512.0]\r\n",
      "#    reinforcement_learning_rate_multiplier: float [Default: 1.0]\r\n",
      "#    reinforcement_learning_train_steps: int [Default: 1000.0]\r\n",
      "#    reward_model_learning_rate_multiplier: float [Default: 1.0]\r\n",
      "#    reward_model_train_steps: int [Default: 1000.0]\r\n",
      "#    target_sequence_length: int [Default: 64.0]\r\n",
      "#    tensorboard_resource_id: str\r\n",
      "# Outputs:\r\n",
      "#    endpoint_resource_name: str\r\n",
      "#    model_resource_name: str\r\n",
      "components:\r\n",
      "  comp-bulkinferrer:\r\n",
      "    executorLabel: exec-bulkinferrer\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          description: Number of accelerators.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          description: Type of accelerator.\r\n",
      "          parameterType: STRING\r\n",
      "        dataset_split:\r\n",
      "          description: Perform inference on this split of the input dataset.\r\n",
      "          parameterType: STRING\r\n",
      "        image_uri:\r\n",
      "          parameterType: STRING\r\n",
      "        input_dataset_path:\r\n",
      "          description: Path to dataset to use for inference.\r\n",
      "          parameterType: STRING\r\n",
      "        input_model:\r\n",
      "          description: Model to use for inference.\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_sequence_length:\r\n",
      "          description: 'Maximum encoder/prefix length. Inputs will be padded\r\n",
      "\r\n",
      "            or truncated to this length.'\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        large_model_reference:\r\n",
      "          description: Predefined model used to create the ``input_model``.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          description: Type of machine.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        sampling_strategy:\r\n",
      "          defaultValue: greedy\r\n",
      "          description: The sampling strategy for inference.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        targets_sequence_length:\r\n",
      "          description: 'Maximum decoder steps. Outputs will be at most this\r\n",
      "\r\n",
      "            length.'\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: 'GCP resources that can be used to track the custom finetuning\r\n",
      "\r\n",
      "            job.'\r\n",
      "          parameterType: STRING\r\n",
      "        output_prediction:\r\n",
      "          description: Where to save the output prediction.\r\n",
      "          parameterType: STRING\r\n",
      "        output_prediction_gcs_path:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-condition-1:\r\n",
      "    dag:\r\n",
      "      tasks:\r\n",
      "        upload-tensorboard-metrics:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-upload-tensorboard-metrics\r\n",
      "          inputs:\r\n",
      "            artifacts:\r\n",
      "              metrics_directory:\r\n",
      "                componentInputArtifact: pipelinechannel--rewardmodeltrainer-tensorboard_metrics\r\n",
      "            parameters:\r\n",
      "              experiment_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: reward-model-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\r\n",
      "              tensorboard_resource_id:\r\n",
      "                componentInputParameter: pipelinechannel--tensorboard_resource_id\r\n",
      "          taskInfo:\r\n",
      "            name: Reward Model Tensorboard Metrics Uploader\r\n",
      "    inputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        pipelinechannel--rewardmodeltrainer-tensorboard_metrics:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "      parameters:\r\n",
      "        pipelinechannel--tensorboard_resource_id:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--value-exists-Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-condition-2:\r\n",
      "    dag:\r\n",
      "      tasks:\r\n",
      "        upload-tensorboard-metrics-2:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-upload-tensorboard-metrics-2\r\n",
      "          inputs:\r\n",
      "            artifacts:\r\n",
      "              metrics_directory:\r\n",
      "                componentInputArtifact: pipelinechannel--reinforcer-tensorboard_metrics\r\n",
      "            parameters:\r\n",
      "              experiment_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: rl-model-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\r\n",
      "              tensorboard_resource_id:\r\n",
      "                componentInputParameter: pipelinechannel--tensorboard_resource_id\r\n",
      "          taskInfo:\r\n",
      "            name: Reinforcement Learning Tensorboard Metrics Uploader\r\n",
      "    inputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        pipelinechannel--reinforcer-tensorboard_metrics:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "      parameters:\r\n",
      "        pipelinechannel--tensorboard_resource_id:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--value-exists-Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-condition-3:\r\n",
      "    dag:\r\n",
      "      tasks:\r\n",
      "        infer-eval-template:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-infer-eval-template\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              instruction:\r\n",
      "                componentInputParameter: pipelinechannel--instruction\r\n",
      "              large_model_reference:\r\n",
      "                componentInputParameter: pipelinechannel--large_model_reference\r\n",
      "              location:\r\n",
      "                componentInputParameter: pipelinechannel--location\r\n",
      "              model_checkpoint:\r\n",
      "                componentInputParameter: pipelinechannel--reinforcer-output_model_path\r\n",
      "              project:\r\n",
      "                componentInputParameter: pipelinechannel--project\r\n",
      "              prompt_dataset:\r\n",
      "                componentInputParameter: pipelinechannel--eval_dataset\r\n",
      "              prompt_sequence_length:\r\n",
      "                componentInputParameter: pipelinechannel--prompt_sequence_length\r\n",
      "              target_sequence_length:\r\n",
      "                componentInputParameter: pipelinechannel--target_sequence_length\r\n",
      "          taskInfo:\r\n",
      "            name: infer-eval-template\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        pipelinechannel--eval_dataset:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--instruction:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--large_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--location:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--project:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--prompt_sequence_length:\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        pipelinechannel--reinforcer-output_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "        pipelinechannel--target_sequence_length:\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        pipelinechannel--value-exists-2-Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-convert-to-delimited-string:\r\n",
      "    executorLabel: exec-convert-to-delimited-string\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        delimiter:\r\n",
      "          defaultValue: ','\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        items:\r\n",
      "          parameterType: LIST\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-create-endpoint-and-deploy-model:\r\n",
      "    executorLabel: exec-create-endpoint-and-deploy-model\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        deploy_model:\r\n",
      "          defaultValue: true\r\n",
      "          description: 'Whether to deploy the model to an endpoint. Default is\r\n",
      "\r\n",
      "            ``True``. If ``False``, the model will not be deployed and output\r\n",
      "\r\n",
      "            artifacts will contain empty strings.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: BOOLEAN\r\n",
      "        display_name:\r\n",
      "          description: Name of the model (shown in Model Registry).\r\n",
      "          parameterType: STRING\r\n",
      "        encryption_spec_key_name:\r\n",
      "          defaultValue: ''\r\n",
      "          description: Customer-managed encryption key.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location for model upload and deployment.\r\n",
      "          parameterType: STRING\r\n",
      "        model_resource_name:\r\n",
      "          description: Path to the created Model on Model Registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Name of the GCP project.\r\n",
      "          parameterType: STRING\r\n",
      "        regional_endpoint:\r\n",
      "          description: Regional API endpoint.\r\n",
      "          parameterType: STRING\r\n",
      "        service_account:\r\n",
      "          defaultValue: ''\r\n",
      "          description: If set, then a custom service account will be used.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        create_endpoint_gcp_resources:\r\n",
      "          description: 'Serialized JSON of GCP resources for\r\n",
      "\r\n",
      "            creating an endpoint.'\r\n",
      "          parameterType: STRING\r\n",
      "        deploy_model_gcp_resources:\r\n",
      "          description: 'Serialized JSON of GCP resources for deploying\r\n",
      "\r\n",
      "            the model.'\r\n",
      "          parameterType: STRING\r\n",
      "        endpoint_resource_name:\r\n",
      "          description: Path to the created endpoint on Online Prediction.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-importer:\r\n",
      "    executorLabel: exec-importer\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        uri:\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        artifact:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "  comp-infer-eval-template:\r\n",
      "    dag:\r\n",
      "      outputs:\r\n",
      "        parameters:\r\n",
      "          output_prediction_gcs_path:\r\n",
      "            valueFromParameter:\r\n",
      "              outputParameterKey: output_prediction_gcs_path\r\n",
      "              producerSubtask: bulkinferrer\r\n",
      "      tasks:\r\n",
      "        bulkinferrer:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-bulkinferrer\r\n",
      "          dependentTasks:\r\n",
      "          - privatetextimporter\r\n",
      "          - resolve-image-uri-2\r\n",
      "          - resolve-machine-spec\r\n",
      "          - resolve-reference-model-metadata\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              accelerator_count:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: accelerator_count\r\n",
      "                  producerTask: resolve-machine-spec\r\n",
      "              accelerator_type:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: accelerator_type\r\n",
      "                  producerTask: resolve-machine-spec\r\n",
      "              dataset_split:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: train\r\n",
      "              image_uri:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: Output\r\n",
      "                  producerTask: resolve-image-uri-2\r\n",
      "              input_dataset_path:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: imported_data_path\r\n",
      "                  producerTask: privatetextimporter\r\n",
      "              input_model:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: reference_model_path\r\n",
      "                  producerTask: resolve-reference-model-metadata\r\n",
      "              inputs_sequence_length:\r\n",
      "                componentInputParameter: prompt_sequence_length\r\n",
      "              large_model_reference:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: large_model_reference\r\n",
      "                  producerTask: resolve-reference-model-metadata\r\n",
      "              location:\r\n",
      "                componentInputParameter: location\r\n",
      "              machine_type:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: machine_type\r\n",
      "                  producerTask: resolve-machine-spec\r\n",
      "              project:\r\n",
      "                componentInputParameter: project\r\n",
      "              sampling_strategy:\r\n",
      "                componentInputParameter: sampling_strategy\r\n",
      "              targets_sequence_length:\r\n",
      "                componentInputParameter: target_sequence_length\r\n",
      "          taskInfo:\r\n",
      "            name: Bulk Inferrer\r\n",
      "        privatetextimporter:\r\n",
      "          cachingOptions: {}\r\n",
      "          componentRef:\r\n",
      "            name: comp-privatetextimporter-2\r\n",
      "          dependentTasks:\r\n",
      "          - resolve-image-uri\r\n",
      "          - resolve-reference-model-metadata\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              image_uri:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: Output\r\n",
      "                  producerTask: resolve-image-uri\r\n",
      "              input_text:\r\n",
      "                componentInputParameter: prompt_dataset\r\n",
      "              inputs_field_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: input_text\r\n",
      "              instruction:\r\n",
      "                componentInputParameter: instruction\r\n",
      "              large_model_reference:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: large_model_reference\r\n",
      "                  producerTask: resolve-reference-model-metadata\r\n",
      "              location:\r\n",
      "                componentInputParameter: location\r\n",
      "              output_split_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: train\r\n",
      "              project:\r\n",
      "                componentInputParameter: project\r\n",
      "              targets_field_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: ''\r\n",
      "          taskInfo:\r\n",
      "            name: Import Prompt Dataset\r\n",
      "        resolve-image-uri:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-resolve-image-uri-5\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              artifact_registry:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: rlhf\r\n",
      "              image_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: text_importer\r\n",
      "              image_name_prefix:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: rlhf_\r\n",
      "              location:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: us\r\n",
      "              project:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: vertex-ai-restricted\r\n",
      "              tag:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: 20231010_1107_RC00\r\n",
      "          taskInfo:\r\n",
      "            name: Resolve Prompt Dataset Image URI\r\n",
      "        resolve-image-uri-2:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-resolve-image-uri-2-2\r\n",
      "          dependentTasks:\r\n",
      "          - resolve-machine-spec\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              accelerator_count:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: accelerator_count\r\n",
      "                  producerTask: resolve-machine-spec\r\n",
      "              accelerator_type:\r\n",
      "                taskOutputParameter:\r\n",
      "                  outputParameterKey: accelerator_type\r\n",
      "                  producerTask: resolve-machine-spec\r\n",
      "              artifact_registry:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: rlhf\r\n",
      "              image_name:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: infer\r\n",
      "              image_name_prefix:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: rlhf_\r\n",
      "              location:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: us\r\n",
      "              project:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: vertex-ai-restricted\r\n",
      "              tag:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: 20231010_1107_RC00\r\n",
      "          taskInfo:\r\n",
      "            name: Resolve Bulk Inferrer Image URI\r\n",
      "        resolve-machine-spec:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-resolve-machine-spec-2\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              location:\r\n",
      "                componentInputParameter: location\r\n",
      "              use_test_spec:\r\n",
      "                runtimeValue:\r\n",
      "                  constant: false\r\n",
      "          taskInfo:\r\n",
      "            name: Resolve Machine Spec\r\n",
      "        resolve-reference-model-metadata:\r\n",
      "          cachingOptions:\r\n",
      "            enableCache: true\r\n",
      "          componentRef:\r\n",
      "            name: comp-resolve-reference-model-metadata-2\r\n",
      "          inputs:\r\n",
      "            parameters:\r\n",
      "              large_model_reference:\r\n",
      "                componentInputParameter: large_model_reference\r\n",
      "              reference_model_path:\r\n",
      "                componentInputParameter: model_checkpoint\r\n",
      "          taskInfo:\r\n",
      "            name: Resolve Model Metadata\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        instruction:\r\n",
      "          description: This field lets the model know what task it needs to perform.\r\n",
      "            Base models have been trained over a large set of varied instructions.\r\n",
      "            You can give a simple and intuitive description of the task and the model\r\n",
      "            will follow it, e.g. \"Classify this movie review as positive or negative\"\r\n",
      "            or \"Translate this sentence to Danish\". Do not specify this if your dataset\r\n",
      "            already prepends the instruction to the inputs field.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        large_model_reference:\r\n",
      "          description: Name of the base model. Supported values are `text-bison@001`,\r\n",
      "            `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\r\n",
      "            are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl`\r\n",
      "            and `t5-xxl` are only supported in `europe-west4`.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          defaultValue: '{{$.pipeline_google_cloud_location}}'\r\n",
      "          description: Location used to run custom jobs. If not specified the location\r\n",
      "            used to run the pipeline will be used.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        model_checkpoint:\r\n",
      "          description: Optional Cloud storage path to the model checkpoint. If not\r\n",
      "            provided, the default checkpoint for the `large_model_reference` will\r\n",
      "            be used.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          defaultValue: '{{$.pipeline_google_cloud_project_id}}'\r\n",
      "          description: Project used to run custom jobs. If not specified the project\r\n",
      "            used to run the pipeline will be used.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        prompt_dataset:\r\n",
      "          description: Cloud storage path to an unlabled prompt dataset used for reinforcement\r\n",
      "            learning. The dataset format is jsonl. Each example in the dataset must\r\n",
      "            have an `input_text` field that contains the prompt.\r\n",
      "          parameterType: STRING\r\n",
      "        prompt_sequence_length:\r\n",
      "          defaultValue: 512.0\r\n",
      "          description: Maximum tokenized sequence length for input text. Higher values\r\n",
      "            increase memory overhead. This value should be at most 8192. Default value\r\n",
      "            is 512.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        sampling_strategy:\r\n",
      "          defaultValue: greedy\r\n",
      "          description: This field specifies the sampling strategy. The valid options\r\n",
      "            are 'greedy' and 'temperature_sampling'.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        target_sequence_length:\r\n",
      "          defaultValue: 64.0\r\n",
      "          description: ' Maximum tokenized sequence length for target text. Higher\r\n",
      "            values increase memory overhead. This value should be at most 1024. Default\r\n",
      "            value is 64.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        output_prediction_gcs_path:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-privatetextcomparisonimporter:\r\n",
      "    executorLabel: exec-privatetextcomparisonimporter\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        choice_field_name:\r\n",
      "          description: 'Name of field that specifies the index of the best\r\n",
      "\r\n",
      "            candidate.'\r\n",
      "          parameterType: STRING\r\n",
      "        comma_separated_candidates_field_names:\r\n",
      "          description: 'Comma separated list of fields that\r\n",
      "\r\n",
      "            contain candidate text, e.g. ``''field_1,field_2,field_3''``.'\r\n",
      "          parameterType: STRING\r\n",
      "        image_uri:\r\n",
      "          description: Location of the text comparison importer image.\r\n",
      "          parameterType: STRING\r\n",
      "        input_text:\r\n",
      "          description: Path to text data. Supports glob patterns.\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_field_name:\r\n",
      "          description: Name of field that contains input text.\r\n",
      "          parameterType: STRING\r\n",
      "        instruction:\r\n",
      "          defaultValue: ''\r\n",
      "          description: Optional instruction to prepend to inputs field.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        large_model_reference:\r\n",
      "          description: 'Predefined model used to create the model to be\r\n",
      "\r\n",
      "            trained. This paramerter is used for obtaining model vocabulary because\r\n",
      "\r\n",
      "            this component tokenizes and then caches the tokenized tasks.'\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          defaultValue: e2-highmem-8\r\n",
      "          description: The type of the machine to provision for the custom job.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        split:\r\n",
      "          description: 'The created seqio task has 1 split, its name is specified\r\n",
      "            by this\r\n",
      "\r\n",
      "            argument.'\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: GCP resources that can be used to track the custom job.\r\n",
      "          parameterType: STRING\r\n",
      "        output_dataset_path:\r\n",
      "          description: Path to cached SeqIO task created from input dataset.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-privatetextimporter:\r\n",
      "    executorLabel: exec-privatetextimporter\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        image_uri:\r\n",
      "          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/rlhf_text_importer_backup:20231010_1107_RC00\r\n",
      "          description: Optional location of the text importer image.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        input_text:\r\n",
      "          description: Path to text data. Supports glob patterns.\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_field_name:\r\n",
      "          description: Name of field that contains input text.\r\n",
      "          parameterType: STRING\r\n",
      "        instruction:\r\n",
      "          defaultValue: ''\r\n",
      "          description: Optional instruction to prepend to inputs field.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        large_model_reference:\r\n",
      "          description: 'Predefined model used to create the model to be\r\n",
      "\r\n",
      "            trained. This paramerter is used for obtaining model vocabulary because\r\n",
      "\r\n",
      "            this component tokenizes and then caches the tokenized tasks.'\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          defaultValue: e2-highmem-8\r\n",
      "          description: The type of the machine to provision for the custom job.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        max_num_input_examples:\r\n",
      "          description: Maximum number of examples to import.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        output_split_name:\r\n",
      "          defaultValue: all\r\n",
      "          description: 'The created seqio task has 1 split, its name is specified\r\n",
      "\r\n",
      "            by this argument.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        targets_field_name:\r\n",
      "          description: Name of field that contains target text.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        imported_data:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Dataset\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "          description: Artifact representing the imported data and cached Tasks.\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: Tracker for GCP resources created by this component.\r\n",
      "          parameterType: STRING\r\n",
      "        imported_data_path:\r\n",
      "          description: Path to cached SeqIO task created from input dataset.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-privatetextimporter-2:\r\n",
      "    executorLabel: exec-privatetextimporter-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        image_uri:\r\n",
      "          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/rlhf_text_importer_backup:20231010_1107_RC00\r\n",
      "          description: Optional location of the text importer image.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        input_text:\r\n",
      "          description: Path to text data. Supports glob patterns.\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_field_name:\r\n",
      "          description: Name of field that contains input text.\r\n",
      "          parameterType: STRING\r\n",
      "        instruction:\r\n",
      "          defaultValue: ''\r\n",
      "          description: Optional instruction to prepend to inputs field.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        large_model_reference:\r\n",
      "          description: 'Predefined model used to create the model to be\r\n",
      "\r\n",
      "            trained. This paramerter is used for obtaining model vocabulary because\r\n",
      "\r\n",
      "            this component tokenizes and then caches the tokenized tasks.'\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          defaultValue: e2-highmem-8\r\n",
      "          description: The type of the machine to provision for the custom job.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        max_num_input_examples:\r\n",
      "          description: Maximum number of examples to import.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        output_split_name:\r\n",
      "          defaultValue: all\r\n",
      "          description: 'The created seqio task has 1 split, its name is specified\r\n",
      "\r\n",
      "            by this argument.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        targets_field_name:\r\n",
      "          description: Name of field that contains target text.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        imported_data:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Dataset\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "          description: Artifact representing the imported data and cached Tasks.\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: Tracker for GCP resources created by this component.\r\n",
      "          parameterType: STRING\r\n",
      "        imported_data_path:\r\n",
      "          description: Path to cached SeqIO task created from input dataset.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-reinforcer:\r\n",
      "    executorLabel: exec-reinforcer\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          description: Number of TPU accelerators.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          description: Type of TPU accelerator. Can be either TPU_V2 or TPU_V3.\r\n",
      "          parameterType: STRING\r\n",
      "        batch_size:\r\n",
      "          defaultValue: 64.0\r\n",
      "          description: Number of examples in each finetuning step. Default is 64.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        image_uri:\r\n",
      "          description: Location of reinforcement learning Docker image.\r\n",
      "          parameterType: STRING\r\n",
      "        input_dataset_path:\r\n",
      "          description: Path to training dataset.\r\n",
      "          parameterType: STRING\r\n",
      "        input_reference_model_path:\r\n",
      "          description: Path to the base model to fine tune.\r\n",
      "          parameterType: STRING\r\n",
      "        input_reward_model_path:\r\n",
      "          description: 'Path to the reward model to use during\r\n",
      "\r\n",
      "            reinforcement learning.'\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_sequence_length:\r\n",
      "          description: Maximum number of input tokens per row.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        kl_coeff:\r\n",
      "          defaultValue: 0.1\r\n",
      "          description: 'Coefficient for KL penalty. This regularizes the policy model\r\n",
      "            and\r\n",
      "\r\n",
      "            penalizes if it diverges from its initial distribution. If set to 0, then\r\n",
      "\r\n",
      "            the reference LM is not loaded into memory.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_DOUBLE\r\n",
      "        large_model_reference:\r\n",
      "          description: 'Predefined model used to create the\r\n",
      "\r\n",
      "            ``input_reference_model``.'\r\n",
      "          parameterType: STRING\r\n",
      "        learning_rate_multiplier:\r\n",
      "          defaultValue: 1.0\r\n",
      "          description: 'Constant multiplied by the base learning rate used\r\n",
      "\r\n",
      "            to adjust the learning rate during reinforcement learning.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_DOUBLE\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        lora_dim:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: 'The rank of the LoRA adapter. If >0, then use LoRA-tuning.\r\n",
      "            If =0,\r\n",
      "\r\n",
      "            then use full-tuning.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        machine_type:\r\n",
      "          description: 'The type of the machine to provision for the custom job. Must\r\n",
      "\r\n",
      "            be a valid GCE instance type and compatible with the accelerator type.'\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        reward_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "        targets_sequence_length:\r\n",
      "          description: Maximum number of target tokens per row.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        train_split:\r\n",
      "          defaultValue: train\r\n",
      "          description: 'Name of the split in the input dataset that contains training\r\n",
      "\r\n",
      "            data. Default is ``''train''``.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        train_steps:\r\n",
      "          description: 'Number of training steps. These are the number of steps\r\n",
      "\r\n",
      "            on top of any steps used to train the base model.'\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "    outputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        tensorboard_metrics:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "          description: Training stats (tensorboard) path.\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: 'GCP resources that can be used to track the custom finetuning\r\n",
      "\r\n",
      "            job.'\r\n",
      "          parameterType: STRING\r\n",
      "        output_adapter_path:\r\n",
      "          description: 'Path to the trained model adapter if LoRA tuning was\r\n",
      "\r\n",
      "            used.'\r\n",
      "          parameterType: STRING\r\n",
      "        output_model_path:\r\n",
      "          description: Path to the trained model checkpoint.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-deploy-model:\r\n",
      "    executorLabel: exec-resolve-deploy-model\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        deploy_model:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "        large_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-resolve-image-uri:\r\n",
      "    executorLabel: exec-resolve-image-uri\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-image-uri-2:\r\n",
      "    executorLabel: exec-resolve-image-uri-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-image-uri-2-2:\r\n",
      "    executorLabel: exec-resolve-image-uri-2-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-image-uri-3:\r\n",
      "    executorLabel: exec-resolve-image-uri-3\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-image-uri-4:\r\n",
      "    executorLabel: exec-resolve-image-uri-4\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-image-uri-5:\r\n",
      "    executorLabel: exec-resolve-image-uri-5\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: Number of accelerators.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          defaultValue: ''\r\n",
      "          description: One of the supported accelerator types, e.g. ``'TPU_V3'``.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        artifact_registry:\r\n",
      "          description: Registry that contains Docker images.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name:\r\n",
      "          description: Base image name, e.g. ``'sft'`` or ``'reward_model'``.\r\n",
      "          parameterType: STRING\r\n",
      "        image_name_prefix:\r\n",
      "          description: Text to prepend to the base image name.\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Region that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project that contains the artifact registry.\r\n",
      "          parameterType: STRING\r\n",
      "        tag:\r\n",
      "          description: Image tag.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-machine-spec:\r\n",
      "    executorLabel: exec-resolve-machine-spec\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        location:\r\n",
      "          description: Where the machine will run.\r\n",
      "          parameterType: STRING\r\n",
      "        use_test_spec:\r\n",
      "          defaultValue: false\r\n",
      "          description: Whether to use a lower resource machine for testing.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: BOOLEAN\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-machine-spec-2:\r\n",
      "    executorLabel: exec-resolve-machine-spec-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        location:\r\n",
      "          description: Where the machine will run.\r\n",
      "          parameterType: STRING\r\n",
      "        use_test_spec:\r\n",
      "          defaultValue: false\r\n",
      "          description: Whether to use a lower resource machine for testing.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: BOOLEAN\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          parameterType: STRING\r\n",
      "        machine_type:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-model-display-name:\r\n",
      "    executorLabel: exec-resolve-model-display-name\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          description: Base model tuned by the pipeline.\r\n",
      "          parameterType: STRING\r\n",
      "        model_display_name:\r\n",
      "          description: 'User-provided display name. If not provided, a default\r\n",
      "\r\n",
      "            display name will be created.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-reference-model-metadata:\r\n",
      "    executorLabel: exec-resolve-reference-model-metadata\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          description: User-provided reference model name.\r\n",
      "          parameterType: STRING\r\n",
      "        reference_model_path:\r\n",
      "          description: 'Optional path to a tuned based model to use in place\r\n",
      "\r\n",
      "            of the default base model. If specified, the model at this path must be\r\n",
      "            a\r\n",
      "\r\n",
      "            tuned version of the base model associated with ``large_model_reference``.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "        reference_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "        reward_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "        reward_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-reference-model-metadata-2:\r\n",
      "    executorLabel: exec-resolve-reference-model-metadata-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          description: User-provided reference model name.\r\n",
      "          parameterType: STRING\r\n",
      "        reference_model_path:\r\n",
      "          description: 'Optional path to a tuned based model to use in place\r\n",
      "\r\n",
      "            of the default base model. If specified, the model at this path must be\r\n",
      "            a\r\n",
      "\r\n",
      "            tuned version of the base model associated with ``large_model_reference``.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "        reference_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "        reward_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "        reward_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-regional-endpoint:\r\n",
      "    executorLabel: exec-resolve-regional-endpoint\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        upload_location:\r\n",
      "          description: Region where the model will be uploaded.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-resolve-upload-model:\r\n",
      "    executorLabel: exec-resolve-upload-model\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        large_model_reference:\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-rewardmodeltrainer:\r\n",
      "    executorLabel: exec-rewardmodeltrainer\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        accelerator_count:\r\n",
      "          description: Number of TPU accelerators.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        accelerator_type:\r\n",
      "          description: Type of TPU accelerator. Can be either TPU_V2 or TPU_V3.\r\n",
      "          parameterType: STRING\r\n",
      "        batch_size:\r\n",
      "          defaultValue: 64.0\r\n",
      "          description: Number of examples in each finetuning step. Default is 64.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        image_uri:\r\n",
      "          description: Location of reward model Docker image.\r\n",
      "          parameterType: STRING\r\n",
      "        input_dataset_path:\r\n",
      "          description: Path to dataset to use to train a reward model.\r\n",
      "          parameterType: STRING\r\n",
      "        input_model_path:\r\n",
      "          description: Path to the base model to fine tune.\r\n",
      "          parameterType: STRING\r\n",
      "        inputs_sequence_length:\r\n",
      "          description: Maximum number of input tokens per row.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        large_model_reference:\r\n",
      "          description: Predefined model used to create the ``input_model``.\r\n",
      "          parameterType: STRING\r\n",
      "        learning_rate_multiplier:\r\n",
      "          defaultValue: 1.0\r\n",
      "          description: 'Constant multiplied by the base learning rate used\r\n",
      "\r\n",
      "            to adjust the learning rate when training a reward model.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_DOUBLE\r\n",
      "        location:\r\n",
      "          description: Location used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        lora_dim:\r\n",
      "          defaultValue: 0.0\r\n",
      "          description: 'The rank of the LoRA adapter. If >0, then use LoRA-tuning.\r\n",
      "            If =0,\r\n",
      "\r\n",
      "            then use full-tuning.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        machine_type:\r\n",
      "          description: 'The type of the machine to provision for the custom job. Must\r\n",
      "\r\n",
      "            be a valid GCE instance type and compatible with the accelerator type.'\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Project used to run the job.\r\n",
      "          parameterType: STRING\r\n",
      "        targets_sequence_length:\r\n",
      "          description: Maximum number of target tokens per row.\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "        train_split:\r\n",
      "          defaultValue: train\r\n",
      "          description: 'Name of the split in the input dataset that contains training\r\n",
      "\r\n",
      "            data. Default is ``''train''``.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        train_steps:\r\n",
      "          description: 'Number of training steps. These are the number of steps\r\n",
      "\r\n",
      "            on top of any steps used to train the base model.'\r\n",
      "          parameterType: NUMBER_INTEGER\r\n",
      "    outputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        tensorboard_metrics:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "          description: Training stats (tensorboard) path.\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: 'GCP resources that can be used to track the custom finetuning\r\n",
      "\r\n",
      "            job.'\r\n",
      "          parameterType: STRING\r\n",
      "        output_model_path:\r\n",
      "          parameterType: STRING\r\n",
      "  comp-upload-llm-model:\r\n",
      "    executorLabel: exec-upload-llm-model\r\n",
      "    inputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        artifact_uri:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "          description: KFP Artifact for adapter.\r\n",
      "      parameters:\r\n",
      "        encryption_spec_key_name:\r\n",
      "          defaultValue: ''\r\n",
      "          description: Customer-managed encryption key.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "        location:\r\n",
      "          description: Location for model upload and deployment.\r\n",
      "          parameterType: STRING\r\n",
      "        model_display_name:\r\n",
      "          description: Name of the model (shown in Model Registry).\r\n",
      "          parameterType: STRING\r\n",
      "        model_reference_name:\r\n",
      "          description: Large model reference name.\r\n",
      "          parameterType: STRING\r\n",
      "        project:\r\n",
      "          description: Name of the GCP project.\r\n",
      "          parameterType: STRING\r\n",
      "        regional_endpoint:\r\n",
      "          description: Regional API endpoint.\r\n",
      "          parameterType: STRING\r\n",
      "        upload_model:\r\n",
      "          defaultValue: true\r\n",
      "          description: 'Whether to upload the model to the Model Registry. Default\r\n",
      "\r\n",
      "            is ``True``. If ``False``, the model will not be uploaded and output\r\n",
      "\r\n",
      "            artifacts will contain empty strings.'\r\n",
      "          isOptional: true\r\n",
      "          parameterType: BOOLEAN\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        gcp_resources:\r\n",
      "          description: Serialized JSON of `gcp_resources`.\r\n",
      "          parameterType: STRING\r\n",
      "        model_resource_name:\r\n",
      "          description: Path to the created Model on Model Registry.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-upload-tensorboard-metrics:\r\n",
      "    executorLabel: exec-upload-tensorboard-metrics\r\n",
      "    inputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        metrics_directory:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "      parameters:\r\n",
      "        experiment_name:\r\n",
      "          description: Name of this tensorboard experiment. Must be unique to a given\r\n",
      "            `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\r\n",
      "          parameterType: STRING\r\n",
      "        tensorboard_resource_id:\r\n",
      "          description: TensorBoard resource ID in the form `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        tensorboard_uri:\r\n",
      "          description: URI of the uploaded tensorboard experiment.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-upload-tensorboard-metrics-2:\r\n",
      "    executorLabel: exec-upload-tensorboard-metrics-2\r\n",
      "    inputDefinitions:\r\n",
      "      artifacts:\r\n",
      "        metrics_directory:\r\n",
      "          artifactType:\r\n",
      "            schemaTitle: system.Artifact\r\n",
      "            schemaVersion: 0.0.1\r\n",
      "      parameters:\r\n",
      "        experiment_name:\r\n",
      "          description: Name of this tensorboard experiment. Must be unique to a given\r\n",
      "            `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\r\n",
      "          parameterType: STRING\r\n",
      "        tensorboard_resource_id:\r\n",
      "          description: TensorBoard resource ID in the form `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        tensorboard_uri:\r\n",
      "          description: URI of the uploaded tensorboard experiment.\r\n",
      "          parameterType: STRING\r\n",
      "  comp-value-exists:\r\n",
      "    executorLabel: exec-value-exists\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        value:\r\n",
      "          description: That might have been provided.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "  comp-value-exists-2:\r\n",
      "    executorLabel: exec-value-exists-2\r\n",
      "    inputDefinitions:\r\n",
      "      parameters:\r\n",
      "        value:\r\n",
      "          description: That might have been provided.\r\n",
      "          isOptional: true\r\n",
      "          parameterType: STRING\r\n",
      "    outputDefinitions:\r\n",
      "      parameters:\r\n",
      "        Output:\r\n",
      "          parameterType: BOOLEAN\r\n",
      "deploymentSpec:\r\n",
      "  executors:\r\n",
      "    exec-bulkinferrer:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"BulkInferrer\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\r\n",
      "          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\r\n",
      "          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\r\n",
      "          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\r\n",
      "          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--input_model={{$.inputs.parameters[''input_model'']}}\",\r\n",
      "          \"--input_dataset={{$.inputs.parameters[''input_dataset_path'']}}\", \"--dataset_split={{$.inputs.parameters[''dataset_split'']}}\",\r\n",
      "          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\r\n",
      "          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\r\n",
      "          \"--sampling_strategy={{$.inputs.parameters[''sampling_strategy'']}}\", \"--output_prediction={{$.outputs.parameters[''output_prediction''].output_file}}\",\r\n",
      "          \"--output_prediction_gcs_path={{$.outputs.parameters[''output_prediction_gcs_path''].output_file}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-convert-to-delimited-string:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - convert_to_delimited_string\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef convert_to_delimited_string(items: List[str], delimiter: str\\\r\n",
      "          \\ = ',') -> str:\\n  \\\"\\\"\\\"Converts a list of strings to single string delimited\\\r\n",
      "          \\ by the specified character.\\\"\\\"\\\"\\n  return delimiter.join(items)\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-create-endpoint-and-deploy-model:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - create_endpoint_and_deploy_model\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef create_endpoint_and_deploy_model(\\n    project: str,\\n    location:\\\r\n",
      "          \\ str,\\n    model_resource_name: str,\\n    display_name: str,\\n    regional_endpoint:\\\r\n",
      "          \\ str,\\n    endpoint_resource_name: dsl.OutputPath(str),\\n    create_endpoint_gcp_resources:\\\r\n",
      "          \\ dsl.OutputPath(str),\\n    deploy_model_gcp_resources: dsl.OutputPath(str),\\n\\\r\n",
      "          \\    encryption_spec_key_name: str = '',\\n    service_account: str = '',\\n\\\r\n",
      "          \\    deploy_model: bool = True,\\n):\\n  \\\"\\\"\\\"Creates a vertex endpoint and\\\r\n",
      "          \\ deploy the specified model.\\n\\n  Args:\\n      project: Name of the GCP\\\r\n",
      "          \\ project.\\n      location: Location for model upload and deployment.\\n\\\r\n",
      "          \\      model_resource_name: Path to the created Model on Model Registry.\\n\\\r\n",
      "          \\      display_name: Name of the model (shown in Model Registry).\\n    \\\r\n",
      "          \\  regional_endpoint: Regional API endpoint.\\n      encryption_spec_key_name:\\\r\n",
      "          \\ Customer-managed encryption key.\\n      service_account: If set, then\\\r\n",
      "          \\ a custom service account will be used.\\n      deploy_model: Whether to\\\r\n",
      "          \\ deploy the model to an endpoint. Default is\\n        ``True``. If ``False``,\\\r\n",
      "          \\ the model will not be deployed and output\\n        artifacts will contain\\\r\n",
      "          \\ empty strings.\\n\\n  Returns:\\n      endpoint_resource_name: Path to the\\\r\n",
      "          \\ created endpoint on Online Prediction.\\n      create_endpoint_gcp_resources:\\\r\n",
      "          \\ Serialized JSON of GCP resources for\\n          creating an endpoint.\\n\\\r\n",
      "          \\      deploy_model_gcp_resources: Serialized JSON of GCP resources for\\\r\n",
      "          \\ deploying\\n          the model.\\n  \\\"\\\"\\\"\\n  import json\\n  import logging\\n\\\r\n",
      "          \\  import os\\n  import sys\\n  from typing import Any, Dict\\n\\n  try:\\n \\\r\n",
      "          \\   from google_cloud_pipeline_components.container.v1.gcp_launcher import\\\r\n",
      "          \\ lro_remote_runner\\n  except ImportError:\\n    from google_cloud_pipeline_components.container.v1.gcp_launcher\\\r\n",
      "          \\ import lro_remote_runner\\n\\n  def run_lro_remote_runner(\\n      url: str,\\\r\n",
      "          \\ payload: Dict[str, Any], gcp_resources: str\\n  ) -> Any:\\n    remote_runner\\\r\n",
      "          \\ = lro_remote_runner.LroRemoteRunner(location)\\n    lro = remote_runner.create_lro(url,\\\r\n",
      "          \\ json.dumps(payload), gcp_resources)\\n    return remote_runner.poll_lro(lro=lro)\\n\\\r\n",
      "          \\n  try:\\n    os.makedirs(os.path.dirname(endpoint_resource_name), exist_ok=True)\\n\\\r\n",
      "          \\n    if not deploy_model:\\n      with open(endpoint_resource_name, 'w')\\\r\n",
      "          \\ as fout:\\n        fout.write('')\\n      return\\n\\n    regional_endpoint\\\r\n",
      "          \\ = regional_endpoint.rstrip('/')\\n\\n    create_endpoint_payload = {\\n \\\r\n",
      "          \\       'displayName': display_name,\\n    }\\n\\n    pipeline_labels_str =\\\r\n",
      "          \\ os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\\n    if pipeline_labels_str:\\n\\\r\n",
      "          \\      create_endpoint_payload['labels'] = json.loads(pipeline_labels_str)\\n\\\r\n",
      "          \\n    if encryption_spec_key_name:\\n      create_endpoint_payload['encryption_spec']\\\r\n",
      "          \\ = {\\n          'kms_key_name': encryption_spec_key_name\\n      }\\n\\n \\\r\n",
      "          \\   create_endpoint_lro = run_lro_remote_runner(\\n        url=(\\n      \\\r\n",
      "          \\      f'{regional_endpoint}/projects/{project}/locations/{location}'\\n\\\r\n",
      "          \\            '/endpoints'\\n        ),\\n        payload=create_endpoint_payload,\\n\\\r\n",
      "          \\        gcp_resources=create_endpoint_gcp_resources,\\n    )\\n\\n    response_endpoint\\\r\n",
      "          \\ = create_endpoint_lro['response']['name']\\n    with open(endpoint_resource_name,\\\r\n",
      "          \\ 'w') as fout:\\n      fout.write(response_endpoint)\\n\\n    logging.info(\\n\\\r\n",
      "          \\        'Endpoint created successfully. Deploying model %s to endpoint',\\n\\\r\n",
      "          \\        model_resource_name,\\n    )\\n\\n    deploy_model_payload = {\\n \\\r\n",
      "          \\       'deployedModel': {\\n            'model': model_resource_name,\\n\\\r\n",
      "          \\            'displayName': display_name,\\n            'automaticResources':\\\r\n",
      "          \\ {'minReplicaCount': 1, 'maxReplicaCount': 1},\\n        }\\n    }\\n    if\\\r\n",
      "          \\ service_account:\\n      deploy_model_payload['deployedModel']['service_account']\\\r\n",
      "          \\ = service_account\\n\\n    _ = run_lro_remote_runner(\\n        url=f'{regional_endpoint}/{response_endpoint}:deployModel',\\n\\\r\n",
      "          \\        payload=deploy_model_payload,\\n        gcp_resources=deploy_model_gcp_resources,\\n\\\r\n",
      "          \\    )\\n\\n    logging.info('Model deployed successfully!')\\n  except Exception\\\r\n",
      "          \\ as e:  # pylint: disable=broad-exception-caught\\n    if isinstance(e,\\\r\n",
      "          \\ ValueError):\\n      raise\\n    logging.exception(str(e))\\n    sys.exit(13)\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-importer:\r\n",
      "      importer:\r\n",
      "        artifactUri:\r\n",
      "          runtimeParameter: uri\r\n",
      "        typeSchema:\r\n",
      "          schemaTitle: system.Artifact\r\n",
      "          schemaVersion: 0.0.1\r\n",
      "    exec-privatetextcomparisonimporter:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"TfdsComparisonImporter\", \"job_spec\": {\"worker_pool_specs\":\r\n",
      "          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\r\n",
      "          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\r\n",
      "          \"args\": [\"--input_text={{$.inputs.parameters[''input_text'']}}\", \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\",\r\n",
      "          \"--comma_separated_candidates_field_names={{$.inputs.parameters[''comma_separated_candidates_field_names'']}}\",\r\n",
      "          \"--choice_field_name={{$.inputs.parameters[''choice_field_name'']}}\", \"--split={{$.inputs.parameters[''split'']}}\",\r\n",
      "          \"--output_cache_dir={{$.outputs.parameters[''output_dataset_path''].output_file}}\",\r\n",
      "          \"--instruction={{$.inputs.parameters[''instruction'']}}\", \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-privatetextimporter:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"TextImporter\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\r\n",
      "          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\r\n",
      "          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\r\n",
      "          \"args\": [\"--input_text={{$.inputs.parameters[''input_text'']}}\", \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\",\r\n",
      "          \"--targets_field_name={{$.inputs.parameters[''targets_field_name'']}}\",\r\n",
      "          \"--output_split_name={{$.inputs.parameters[''output_split_name'']}}\", \"--instruction={{$.inputs.parameters[''instruction'']}}\",\r\n",
      "          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\r\n",
      "          \"--output_dataset_path={{$.pipeline_root}}{{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\r\n",
      "          \"--imported_data_path={{$.outputs.parameters[''imported_data_path''].output_file}}\",\r\n",
      "          \"--max_num_input_examples={{$.inputs.parameters[''max_num_input_examples'']}}\",\r\n",
      "          \"--executor_input={{$.json_escape[1]}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-privatetextimporter-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"TextImporter\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\r\n",
      "          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\r\n",
      "          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\r\n",
      "          \"args\": [\"--input_text={{$.inputs.parameters[''input_text'']}}\", \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\",\r\n",
      "          \"--targets_field_name={{$.inputs.parameters[''targets_field_name'']}}\",\r\n",
      "          \"--output_split_name={{$.inputs.parameters[''output_split_name'']}}\", \"--instruction={{$.inputs.parameters[''instruction'']}}\",\r\n",
      "          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\r\n",
      "          \"--output_dataset_path={{$.pipeline_root}}{{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\r\n",
      "          \"--imported_data_path={{$.outputs.parameters[''imported_data_path''].output_file}}\",\r\n",
      "          \"--max_num_input_examples={{$.inputs.parameters[''max_num_input_examples'']}}\",\r\n",
      "          \"--executor_input={{$.json_escape[1]}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-reinforcer:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"Reinforcer\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\r\n",
      "          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\r\n",
      "          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\r\n",
      "          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\r\n",
      "          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--input_reference_model_path={{$.inputs.parameters[''input_reference_model_path'']}}\",\r\n",
      "          \"--input_reward_model_path={{$.inputs.parameters[''input_reward_model_path'']}}\",\r\n",
      "          \"--input_dataset_path={{$.inputs.parameters[''input_dataset_path'']}}\",\r\n",
      "          \"--train_steps={{$.inputs.parameters[''train_steps'']}}\", \"--output_model_path={{$.outputs.parameters[''output_model_path''].output_file}}\",\r\n",
      "          \"--output_adapter_path={{$.outputs.parameters[''output_adapter_path''].output_file}}\",\r\n",
      "          \"--tensorboard_metrics_path={{$.outputs.artifacts[''tensorboard_metrics''].path}}\",\r\n",
      "          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--reward_model_reference={{$.inputs.parameters[''reward_model_reference'']}}\",\r\n",
      "          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\r\n",
      "          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\r\n",
      "          \"--train_split={{$.inputs.parameters[''train_split'']}}\", \"--batch_size={{$.inputs.parameters[''batch_size'']}}\",\r\n",
      "          \"--learning_rate_multiplier={{$.inputs.parameters[''learning_rate_multiplier'']}}\",\r\n",
      "          \"--kl_coeff={{$.inputs.parameters[''kl_coeff'']}}\", \"--lora_dim={{$.inputs.parameters[''lora_dim'']}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-deploy-model:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_deploy_model\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_deploy_model(\\n    deploy_model: bool, large_model_reference:\\\r\n",
      "          \\ str\\n) -> bool:\\n  \\\"\\\"\\\"Resolves runtime parameter that determines whether\\\r\n",
      "          \\ the tuned model should be deployed.\\\"\\\"\\\"\\n  supported_models = {'BISON'}\\n\\\r\n",
      "          \\  if deploy_model and large_model_reference in supported_models:\\n    return\\\r\n",
      "          \\ True\\n  return False\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri-2-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri-3:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri-4:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-image-uri-5:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_image_uri\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_image_uri(\\n    image_name: str,\\n    project: str,\\n\\\r\n",
      "          \\    location: str,\\n    artifact_registry: str,\\n    image_name_prefix:\\\r\n",
      "          \\ str,\\n    tag: str,\\n    accelerator_type: str = '',\\n    accelerator_count:\\\r\n",
      "          \\ int = 0,\\n) -> str:\\n  \\\"\\\"\\\"Generates image uri based on base image name\\\r\n",
      "          \\ and accelerator type.\\n\\n  Args:\\n    image_name: Base image name, e.g.\\\r\n",
      "          \\ ``'sft'`` or ``'reward_model'``.\\n    project: Project that contains the\\\r\n",
      "          \\ artifact registry.\\n    location: Region that contains the artifact registry.\\n\\\r\n",
      "          \\    artifact_registry: Registry that contains Docker images.\\n    image_name_prefix:\\\r\n",
      "          \\ Text to prepend to the base image name.\\n    tag: Image tag.\\n    accelerator_type:\\\r\n",
      "          \\ One of the supported accelerator types, e.g. ``'TPU_V3'``.\\n    accelerator_count:\\\r\n",
      "          \\ Number of accelerators.\\n\\n  Returns:\\n    Docker image uri\\n\\n  Raises:\\n\\\r\n",
      "          \\    ValueError: if an unsupported accelerator type is provided.\\n  \\\"\\\"\\\r\n",
      "          \\\"\\n  cpu_only_images = {\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n\\n  if image_name in cpu_only_images:\\n    accelerator_postfix = ''\\n\\\r\n",
      "          \\  elif accelerator_type == 'TPU_V3':\\n    accelerator_postfix = '_tpu'\\n\\\r\n",
      "          \\  elif accelerator_type == 'NVIDIA_A100_80GB' and accelerator_count ==\\\r\n",
      "          \\ 8:\\n    accelerator_postfix = '_gpu_test'\\n  else:\\n    accelerator_postfix\\\r\n",
      "          \\ = '_gpu'\\n\\n  backup_images = {\\n      'sft',\\n      'reward_model',\\n\\\r\n",
      "          \\      'reinforcer',\\n      'infer',\\n      'text_importer',\\n      'text_comparison_importer',\\n\\\r\n",
      "          \\  }\\n  if image_name in backup_images and accelerator_postfix != '_gpu_test':\\n\\\r\n",
      "          \\    accelerator_postfix += '_backup'\\n  return f'{location}-docker.pkg.dev/{project}/{artifact_registry}/{image_name_prefix}{image_name}{accelerator_postfix}:{tag}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-machine-spec:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_machine_spec\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_machine_spec(\\n    location: str,\\n    use_test_spec:\\\r\n",
      "          \\ bool = False,\\n) -> NamedTuple(\\n    'MachineSpec', machine_type=str,\\\r\n",
      "          \\ accelerator_type=str, accelerator_count=int\\n):\\n  \\\"\\\"\\\"Returns machine\\\r\n",
      "          \\ spec to use for a given location.\\n\\n  Args:\\n    location: Where the\\\r\n",
      "          \\ machine will run.\\n    use_test_spec: Whether to use a lower resource\\\r\n",
      "          \\ machine for testing.\\n\\n  Returns:\\n    Machine spec.\\n\\n  Raises:\\n \\\r\n",
      "          \\   ValueError: If accelerators are requested in an unsupported location.\\n\\\r\n",
      "          \\  \\\"\\\"\\\"\\n  outputs = NamedTuple(\\n      'MachineSpec',\\n      machine_type=str,\\n\\\r\n",
      "          \\      accelerator_type=str,\\n      accelerator_count=int,\\n  )\\n  tpu_regions\\\r\n",
      "          \\ = {'europe-west4'}\\n  gpu_regions = {'us-central1'}\\n  if use_test_spec:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='a2-highgpu-1g',\\n        accelerator_type='NVIDIA_TESLA_A100',\\n\\\r\n",
      "          \\        accelerator_count=1,\\n    )\\n  elif location in tpu_regions:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='cloud-tpu',\\n        accelerator_type='TPU_V3',\\n\\\r\n",
      "          \\        accelerator_count=64,\\n    )\\n  elif location in gpu_regions:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='a2-ultragpu-8g',\\n        accelerator_type='NVIDIA_A100_80GB',\\n\\\r\n",
      "          \\        accelerator_count=8,\\n    )\\n  raise ValueError(\\n      f'Unsupported\\\r\n",
      "          \\ accelerator location {location}. Must be one of'\\n      f' {tpu_regions\\\r\n",
      "          \\ | gpu_regions}.'\\n  )\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-machine-spec-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_machine_spec\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_machine_spec(\\n    location: str,\\n    use_test_spec:\\\r\n",
      "          \\ bool = False,\\n) -> NamedTuple(\\n    'MachineSpec', machine_type=str,\\\r\n",
      "          \\ accelerator_type=str, accelerator_count=int\\n):\\n  \\\"\\\"\\\"Returns machine\\\r\n",
      "          \\ spec to use for a given location.\\n\\n  Args:\\n    location: Where the\\\r\n",
      "          \\ machine will run.\\n    use_test_spec: Whether to use a lower resource\\\r\n",
      "          \\ machine for testing.\\n\\n  Returns:\\n    Machine spec.\\n\\n  Raises:\\n \\\r\n",
      "          \\   ValueError: If accelerators are requested in an unsupported location.\\n\\\r\n",
      "          \\  \\\"\\\"\\\"\\n  outputs = NamedTuple(\\n      'MachineSpec',\\n      machine_type=str,\\n\\\r\n",
      "          \\      accelerator_type=str,\\n      accelerator_count=int,\\n  )\\n  tpu_regions\\\r\n",
      "          \\ = {'europe-west4'}\\n  gpu_regions = {'us-central1'}\\n  if use_test_spec:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='a2-highgpu-1g',\\n        accelerator_type='NVIDIA_TESLA_A100',\\n\\\r\n",
      "          \\        accelerator_count=1,\\n    )\\n  elif location in tpu_regions:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='cloud-tpu',\\n        accelerator_type='TPU_V3',\\n\\\r\n",
      "          \\        accelerator_count=64,\\n    )\\n  elif location in gpu_regions:\\n\\\r\n",
      "          \\    return outputs(\\n        machine_type='a2-ultragpu-8g',\\n        accelerator_type='NVIDIA_A100_80GB',\\n\\\r\n",
      "          \\        accelerator_count=8,\\n    )\\n  raise ValueError(\\n      f'Unsupported\\\r\n",
      "          \\ accelerator location {location}. Must be one of'\\n      f' {tpu_regions\\\r\n",
      "          \\ | gpu_regions}.'\\n  )\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-model-display-name:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_model_display_name\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_model_display_name(\\n    large_model_reference: str,\\n\\\r\n",
      "          \\    model_display_name: Optional[str] = None,\\n) -> str:\\n  \\\"\\\"\\\"Gets\\\r\n",
      "          \\ the model display name shown in the registry and used for endpoints.\\n\\\r\n",
      "          \\n  Args:\\n    large_model_reference: Base model tuned by the pipeline.\\n\\\r\n",
      "          \\    model_display_name: User-provided display name. If not provided, a\\\r\n",
      "          \\ default\\n      display name will be created.\\n\\n  Returns:\\n    Either\\\r\n",
      "          \\ the user-provided name or a default display name with the form\\n    ``{large_model_reference}-{timestamp}``\\n\\\r\n",
      "          \\  \\\"\\\"\\\"\\n  # pylint: disable=g-import-not-at-top\\n  import datetime\\n\\\r\n",
      "          \\  # pylint: enable=g-import-not-at-top\\n  now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\\n\\\r\n",
      "          \\  return model_display_name or f'{large_model_reference.lower()}-{now}'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-reference-model-metadata:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_reference_model_metadata\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_reference_model_metadata(\\n    large_model_reference:\\\r\n",
      "          \\ str,\\n    reference_model_path: Optional[str] = None,\\n) -> NamedTuple(\\n\\\r\n",
      "          \\    'Outputs',\\n    large_model_reference=str,\\n    reference_model_path=str,\\n\\\r\n",
      "          \\    reward_model_reference=str,\\n    reward_model_path=str,\\n):\\n  \\\"\\\"\\\r\n",
      "          \\\"Resolves reference model metadata needed by downstream components.\\n\\n\\\r\n",
      "          \\  Args:\\n    large_model_reference: User-provided reference model name.\\n\\\r\n",
      "          \\    reference_model_path: Optional path to a tuned based model to use in\\\r\n",
      "          \\ place\\n      of the default base model. If specified, the model at this\\\r\n",
      "          \\ path must be a\\n      tuned version of the base model associated with\\\r\n",
      "          \\ ``large_model_reference``.\\n\\n  Returns:\\n    Base model name (used by\\\r\n",
      "          \\ downstream components to find gin configs and load\\n    vocabularies)\\\r\n",
      "          \\ and the path to the base model checkpoint.\\n\\n  Raises:\\n    ValueError:\\\r\n",
      "          \\ if no metadata exists for the given base model.\\n  \\\"\\\"\\\"\\n  reference_model_metadata\\\r\n",
      "          \\ = NamedTuple(\\n      'ReferenceModelMetadata',\\n      large_model_reference=str,\\n\\\r\n",
      "          \\      reference_model_path=str,\\n      reward_model_reference=str,\\n  \\\r\n",
      "          \\    reward_model_path=str,\\n      is_supported=bool,\\n  )\\n\\n  reference_models\\\r\n",
      "          \\ = {\\n      't5-small': reference_model_metadata(\\n          large_model_reference='T5_SMALL',\\n\\\r\n",
      "          \\          reference_model_path=(\\n              'gs://t5-data/pretrained_models/t5x/flan_t5_small/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='T5_SMALL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_small',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-large': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_LARGE',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://t5-data/pretrained_models/t5x/flan_t5_large/'\\n   \\\r\n",
      "          \\       ),\\n          reward_model_reference='T5_LARGE',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_large',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-xl': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_XL',\\n          reference_model_path='gs://t5-data/pretrained_models/t5x/flan_t5_xl/',\\n\\\r\n",
      "          \\          reward_model_reference='T5_XL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_xl',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-xxl': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_XXL',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://t5-data/pretrained_models/t5x/flan_t5_xxl/'\\n     \\\r\n",
      "          \\     ),\\n          reward_model_reference='T5_XL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_xl',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'palm-tiny': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='PALM_TINY',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_palm_tiny/',\\n\\\r\n",
      "          \\          reward_model_reference='PALM_TINY',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_palm_tiny/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'gecko': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='GECKO',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_gecko/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='GECKO',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_gecko_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'otter': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='OTTER',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'bison': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='BISON',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,  # Deprecated: Use text-bision@001 instead.\\n\\\r\n",
      "          \\      ),\\n      'text-bison@001': reference_model_metadata(\\n         \\\r\n",
      "          \\ large_model_reference='BISON',\\n          reference_model_path=(\\n   \\\r\n",
      "          \\           'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'chat-bison@001': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='BISON',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'elephant': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='ELEPHANT',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_elephant/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'llama-2-7b': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_7B',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_7B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-13b': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_13B',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_13B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-7b-chat': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_7B_CHAT',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b_chat/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_7B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-13b-chat': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_13B_CHAT',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b_chat/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_13B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n  }\\n\\n  reference_model_key =\\\r\n",
      "          \\ large_model_reference.lower().replace('_', '-')\\n  if reference_model_key\\\r\n",
      "          \\ not in reference_models:\\n    supported_models = [\\n        k for k, v\\\r\n",
      "          \\ in reference_models.items() if v.is_supported\\n    ]\\n    raise ValueError(\\n\\\r\n",
      "          \\        f'Unknown reference model {large_model_reference}.'\\n        '\\\r\n",
      "          \\ large_model_reference must be one of'\\n        f' {sorted(supported_models)}.'\\n\\\r\n",
      "          \\    )\\n\\n  reference_model = reference_models[reference_model_key]\\n\\n\\\r\n",
      "          \\  outputs = NamedTuple(\\n      'Outputs',\\n      large_model_reference=str,\\n\\\r\n",
      "          \\      reference_model_path=str,\\n      reward_model_reference=str,\\n  \\\r\n",
      "          \\    reward_model_path=str,\\n  )\\n\\n  return outputs(\\n      large_model_reference=reference_model.large_model_reference,\\n\\\r\n",
      "          \\      reference_model_path=(\\n          reference_model_path or reference_model.reference_model_path\\n\\\r\n",
      "          \\      ),\\n      reward_model_reference=reference_model.reward_model_reference,\\n\\\r\n",
      "          \\      reward_model_path=reference_model.reward_model_path,\\n  )\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-reference-model-metadata-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_reference_model_metadata\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_reference_model_metadata(\\n    large_model_reference:\\\r\n",
      "          \\ str,\\n    reference_model_path: Optional[str] = None,\\n) -> NamedTuple(\\n\\\r\n",
      "          \\    'Outputs',\\n    large_model_reference=str,\\n    reference_model_path=str,\\n\\\r\n",
      "          \\    reward_model_reference=str,\\n    reward_model_path=str,\\n):\\n  \\\"\\\"\\\r\n",
      "          \\\"Resolves reference model metadata needed by downstream components.\\n\\n\\\r\n",
      "          \\  Args:\\n    large_model_reference: User-provided reference model name.\\n\\\r\n",
      "          \\    reference_model_path: Optional path to a tuned based model to use in\\\r\n",
      "          \\ place\\n      of the default base model. If specified, the model at this\\\r\n",
      "          \\ path must be a\\n      tuned version of the base model associated with\\\r\n",
      "          \\ ``large_model_reference``.\\n\\n  Returns:\\n    Base model name (used by\\\r\n",
      "          \\ downstream components to find gin configs and load\\n    vocabularies)\\\r\n",
      "          \\ and the path to the base model checkpoint.\\n\\n  Raises:\\n    ValueError:\\\r\n",
      "          \\ if no metadata exists for the given base model.\\n  \\\"\\\"\\\"\\n  reference_model_metadata\\\r\n",
      "          \\ = NamedTuple(\\n      'ReferenceModelMetadata',\\n      large_model_reference=str,\\n\\\r\n",
      "          \\      reference_model_path=str,\\n      reward_model_reference=str,\\n  \\\r\n",
      "          \\    reward_model_path=str,\\n      is_supported=bool,\\n  )\\n\\n  reference_models\\\r\n",
      "          \\ = {\\n      't5-small': reference_model_metadata(\\n          large_model_reference='T5_SMALL',\\n\\\r\n",
      "          \\          reference_model_path=(\\n              'gs://t5-data/pretrained_models/t5x/flan_t5_small/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='T5_SMALL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_small',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-large': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_LARGE',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://t5-data/pretrained_models/t5x/flan_t5_large/'\\n   \\\r\n",
      "          \\       ),\\n          reward_model_reference='T5_LARGE',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_large',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-xl': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_XL',\\n          reference_model_path='gs://t5-data/pretrained_models/t5x/flan_t5_xl/',\\n\\\r\n",
      "          \\          reward_model_reference='T5_XL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_xl',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      't5-xxl': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='T5_XXL',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://t5-data/pretrained_models/t5x/flan_t5_xxl/'\\n     \\\r\n",
      "          \\     ),\\n          reward_model_reference='T5_XL',\\n          reward_model_path='gs://t5-data/pretrained_models/t5x/t5_1_1_xl',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'palm-tiny': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='PALM_TINY',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_palm_tiny/',\\n\\\r\n",
      "          \\          reward_model_reference='PALM_TINY',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_palm_tiny/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'gecko': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='GECKO',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_gecko/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='GECKO',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_gecko_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'otter': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='OTTER',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'bison': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='BISON',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,  # Deprecated: Use text-bision@001 instead.\\n\\\r\n",
      "          \\      ),\\n      'text-bison@001': reference_model_metadata(\\n         \\\r\n",
      "          \\ large_model_reference='BISON',\\n          reference_model_path=(\\n   \\\r\n",
      "          \\           'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'chat-bison@001': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='BISON',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_bison/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'elephant': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='ELEPHANT',\\n          reference_model_path=(\\n\\\r\n",
      "          \\              'gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_elephant/'\\n\\\r\n",
      "          \\          ),\\n          reward_model_reference='OTTER',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/palm/t5x_otter_pretrain/',\\n\\\r\n",
      "          \\          is_supported=False,\\n      ),\\n      'llama-2-7b': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_7B',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_7B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-13b': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_13B',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_13B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-7b-chat': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_7B_CHAT',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b_chat/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_7B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_7b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n      'llama-2-13b-chat': reference_model_metadata(\\n\\\r\n",
      "          \\          large_model_reference='LLAMA_2_13B_CHAT',\\n          reference_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b_chat/',\\n\\\r\n",
      "          \\          reward_model_reference='LLAMA_2_13B',\\n          reward_model_path='gs://vertex-rlhf-restricted/pretrained_models/llama/t5x_llama_2_13b/',\\n\\\r\n",
      "          \\          is_supported=True,\\n      ),\\n  }\\n\\n  reference_model_key =\\\r\n",
      "          \\ large_model_reference.lower().replace('_', '-')\\n  if reference_model_key\\\r\n",
      "          \\ not in reference_models:\\n    supported_models = [\\n        k for k, v\\\r\n",
      "          \\ in reference_models.items() if v.is_supported\\n    ]\\n    raise ValueError(\\n\\\r\n",
      "          \\        f'Unknown reference model {large_model_reference}.'\\n        '\\\r\n",
      "          \\ large_model_reference must be one of'\\n        f' {sorted(supported_models)}.'\\n\\\r\n",
      "          \\    )\\n\\n  reference_model = reference_models[reference_model_key]\\n\\n\\\r\n",
      "          \\  outputs = NamedTuple(\\n      'Outputs',\\n      large_model_reference=str,\\n\\\r\n",
      "          \\      reference_model_path=str,\\n      reward_model_reference=str,\\n  \\\r\n",
      "          \\    reward_model_path=str,\\n  )\\n\\n  return outputs(\\n      large_model_reference=reference_model.large_model_reference,\\n\\\r\n",
      "          \\      reference_model_path=(\\n          reference_model_path or reference_model.reference_model_path\\n\\\r\n",
      "          \\      ),\\n      reward_model_reference=reference_model.reward_model_reference,\\n\\\r\n",
      "          \\      reward_model_path=reference_model.reward_model_path,\\n  )\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-regional-endpoint:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_regional_endpoint\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_regional_endpoint(upload_location: str) -> str:\\n  \\\"\\\r\n",
      "          \\\"\\\"Gets the regional endpoint used to upload a model to the registry.\\n\\\r\n",
      "          \\n  Args:\\n    upload_location: Region where the model will be uploaded.\\n\\\r\n",
      "          \\n  Returns:\\n    Regional endpoint.\\n  \\\"\\\"\\\"\\n  return f'https://{upload_location}-aiplatform.googleapis.com/ui'\\n\\\r\n",
      "          \\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-resolve-upload-model:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - resolve_upload_model\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef resolve_upload_model(large_model_reference: str) -> bool:\\n \\\r\n",
      "          \\ \\\"\\\"\\\"Returns whether the model should be uploaded.\\\"\\\"\\\"\\n  supported_models\\\r\n",
      "          \\ = {'BISON'}\\n  if large_model_reference in supported_models:\\n    return\\\r\n",
      "          \\ True\\n  return False\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-rewardmodeltrainer:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --type\r\n",
      "        - CustomJob\r\n",
      "        - --payload\r\n",
      "        - '{\"display_name\": \"RewardModelTrainer\", \"job_spec\": {\"worker_pool_specs\":\r\n",
      "          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\r\n",
      "          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\r\n",
      "          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\r\n",
      "          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--train_steps={{$.inputs.parameters[''train_steps'']}}\",\r\n",
      "          \"--input_model_path={{$.inputs.parameters[''input_model_path'']}}\", \"--input_dataset_path={{$.inputs.parameters[''input_dataset_path'']}}\",\r\n",
      "          \"--output_model_path={{$.outputs.parameters[''output_model_path''].output_file}}\",\r\n",
      "          \"--tensorboard_metrics_path={{$.outputs.artifacts[''tensorboard_metrics''].path}}\",\r\n",
      "          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\r\n",
      "          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\r\n",
      "          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\r\n",
      "          \"--train_split={{$.inputs.parameters[''train_split'']}}\", \"--batch_size={{$.inputs.parameters[''batch_size'']}}\",\r\n",
      "          \"--learning_rate_multiplier={{$.inputs.parameters[''learning_rate_multiplier'']}}\",\r\n",
      "          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\r\n",
      "          \"--lora_dim={{$.inputs.parameters[''lora_dim'']}}\"]}}]}}'\r\n",
      "        - --project\r\n",
      "        - '{{$.inputs.parameters[''project'']}}'\r\n",
      "        - --location\r\n",
      "        - '{{$.inputs.parameters[''location'']}}'\r\n",
      "        - --gcp_resources\r\n",
      "        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\r\n",
      "        command:\r\n",
      "        - python3\r\n",
      "        - -u\r\n",
      "        - -m\r\n",
      "        - google_cloud_pipeline_components.container.v1.custom_job.launcher\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-upload-llm-model:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - upload_llm_model\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef upload_llm_model(\\n    project: str,\\n    location: str,\\n  \\\r\n",
      "          \\  artifact_uri: dsl.Input[dsl.Artifact],\\n    model_reference_name: str,\\n\\\r\n",
      "          \\    model_display_name: str,\\n    regional_endpoint: str,\\n    model_resource_name:\\\r\n",
      "          \\ dsl.OutputPath(str),\\n    gcp_resources: dsl.OutputPath(str),\\n    encryption_spec_key_name:\\\r\n",
      "          \\ str = '',\\n    upload_model: bool = True,\\n):\\n  \\\"\\\"\\\"Uploads LLM model.\\n\\\r\n",
      "          \\n  Args:\\n      project: Name of the GCP project.\\n      location: Location\\\r\n",
      "          \\ for model upload and deployment.\\n      artifact_uri: KFP Artifact for\\\r\n",
      "          \\ adapter.\\n      model_reference_name: Large model reference name.\\n  \\\r\n",
      "          \\    model_display_name: Name of the model (shown in Model Registry).\\n\\\r\n",
      "          \\      regional_endpoint: Regional API endpoint.\\n      encryption_spec_key_name:\\\r\n",
      "          \\ Customer-managed encryption key.\\n      upload_model: Whether to upload\\\r\n",
      "          \\ the model to the Model Registry. Default\\n        is ``True``. If ``False``,\\\r\n",
      "          \\ the model will not be uploaded and output\\n        artifacts will contain\\\r\n",
      "          \\ empty strings.\\n\\n  Returns:\\n      model_resource_name: Path to the created\\\r\n",
      "          \\ Model on Model Registry.\\n      gcp_resources: Serialized JSON of `gcp_resources`.\\n\\\r\n",
      "          \\  \\\"\\\"\\\"\\n  import json\\n  import logging\\n  import os\\n  import sys\\n\\n\\\r\n",
      "          \\  try:\\n    from google_cloud_pipeline_components.container.v1.gcp_launcher\\\r\n",
      "          \\ import lro_remote_runner\\n  except ImportError:\\n    from google_cloud_pipeline_components.container.v1.gcp_launcher\\\r\n",
      "          \\ import lro_remote_runner\\n\\n  try:\\n    os.makedirs(os.path.dirname(model_resource_name),\\\r\n",
      "          \\ exist_ok=True)\\n\\n    if not upload_model:\\n      with open(model_resource_name,\\\r\n",
      "          \\ 'w') as fout:\\n        fout.write('')\\n      return\\n\\n    pipeline_labels_str\\\r\n",
      "          \\ = os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\\n    labels = json.loads(pipeline_labels_str)\\\r\n",
      "          \\ if pipeline_labels_str else {}\\n    labels['google-vertex-llm-tuning-base-model-id']\\\r\n",
      "          \\ = (\\n        model_reference_name.replace('@', '-')\\n    )\\n\\n    model_upload_payload\\\r\n",
      "          \\ = {\\n        'model': {\\n            'displayName': model_display_name,\\n\\\r\n",
      "          \\            'largeModelReference': {'name': model_reference_name},\\n  \\\r\n",
      "          \\          'labels': labels,\\n            'generatedModelSource': {'genie_source':\\\r\n",
      "          \\ {'base_model_uri': ''}},\\n            'artifactUri': artifact_uri.uri,\\n\\\r\n",
      "          \\        }\\n    }\\n    if encryption_spec_key_name:\\n      model_upload_payload['model']['encryption_spec']\\\r\n",
      "          \\ = {\\n          'kms_key_name': encryption_spec_key_name\\n      }\\n\\n \\\r\n",
      "          \\   regional_endpoint = regional_endpoint.rstrip('/')\\n    upload_model_uri\\\r\n",
      "          \\ = (\\n        f'{regional_endpoint}/projects/{project}/locations/{location}/models:'\\n\\\r\n",
      "          \\        'upload'\\n    )\\n\\n    remote_runner = lro_remote_runner.LroRemoteRunner(location)\\n\\\r\n",
      "          \\    upload_model_lro = remote_runner.create_lro(\\n        upload_model_uri,\\n\\\r\n",
      "          \\        json.dumps(model_upload_payload),\\n        gcp_resources,\\n   \\\r\n",
      "          \\ )\\n    upload_model_lro = remote_runner.poll_lro(lro=upload_model_lro)\\n\\\r\n",
      "          \\    model_resource = upload_model_lro['response']['model']\\n    model_version_id\\\r\n",
      "          \\ = upload_model_lro['response'].get(\\n        'model_version_id'\\n    )\\\r\n",
      "          \\ or upload_model_lro['response'].get('modelVersionId')\\n    if model_version_id:\\n\\\r\n",
      "          \\      model_resource += f'@{model_version_id}'\\n\\n    with open(model_resource_name,\\\r\n",
      "          \\ 'w') as fout:\\n      fout.write(model_resource)\\n\\n  except Exception\\\r\n",
      "          \\ as e:  # pylint: disable=broad-exception-caught\\n    if isinstance(e,\\\r\n",
      "          \\ ValueError):\\n      raise\\n    logging.exception(str(e))\\n    sys.exit(13)\\n\\\r\n",
      "          \\n\"\r\n",
      "        env:\r\n",
      "        - name: VERTEX_AI_PIPELINES_RUN_LABELS\r\n",
      "          value: '{\"tune-type\": \"rlhf\"}'\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-upload-tensorboard-metrics:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - '{{$.inputs.parameters[''tensorboard_resource_id'']}}'\r\n",
      "        - '{{$.inputs.artifacts[''metrics_directory''].path}}'\r\n",
      "        - '{{$.inputs.parameters[''experiment_name'']}}'\r\n",
      "        - '{{$.outputs.parameters[''tensorboard_uri''].output_file}}'\r\n",
      "        command:\r\n",
      "        - bash\r\n",
      "        - -c\r\n",
      "        - \"\\nset -e -x\\nTENSORBOARD_RESOURCE_ID=\\\"$0\\\"\\nMETRICS_DIRECTORY_URI=\\\"$1\\\"\\\r\n",
      "          \\nEXPERIMENT_NAME=\\\"$2\\\"\\nTENSORBOARD_URI=\\\"$3\\\"\\n\\nmkdir -p \\\"$(dirname\\\r\n",
      "          \\ ${TENSORBOARD_URI})\\\"\\nif [ -z \\\"${TENSORBOARD_RESOURCE_ID}\\\" ];\\nthen\\n\\\r\n",
      "          \\  echo \\\"TensorBoard ID is not set. Skip uploading the TensorBoard.\\\"\\n\\\r\n",
      "          \\  echo -n \\\"\\\" > \\\"${TENSORBOARD_URI}\\\"\\n  exit 0\\nfi\\n\\nif [ -z \\\"${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\ ]; then\\n  echo \\\"Metrics directory uri is not set.\\\"\\n  exit 1\\nelif\\\r\n",
      "          \\ [ -z \\\"${EXPERIMENT_NAME}\\\" ]; then\\n  echo \\\"Experiment name is not set.\\\"\\\r\n",
      "          \\n  exit 1\\nelif [ -z \\\"${TENSORBOARD_URI}\\\" ]; then\\n  echo \\\"TensorBoard\\\r\n",
      "          \\ URI is not set.\\\"\\n  exit 1\\nfi\\n\\ncase \\\"${METRICS_DIRECTORY_URI}\\\" in\\n\\\r\n",
      "          \\  \\\"gs://\\\"*) ;;\\n  \\\"/gcs/\\\"*)\\n    METRICS_DIRECTORY_URI=${METRICS_DIRECTORY_URI/\\\"\\\r\n",
      "          /gcs/\\\"/\\\"gs://\\\"}\\n    echo \\\"Replaced /gcs/ path with ${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\n    ;;\\n  *)\\n    echo \\\"Invalid metrics directory uri. Metrics directory\\\r\n",
      "          \\ uri must start with gs:// or /gcs/.\\\"\\n    exit 1\\n    ;;\\nesac\\n\\nif\\\r\n",
      "          \\ [[ \\\"${TENSORBOARD_RESOURCE_ID}\\\" =~ ^projects/[^/]+/locations/[^/]+/tensorboards/[0-9]+$\\\r\n",
      "          \\ ]]; then\\n  echo \\\"Split tensorboard resource id\\\"\\n  TENSORBOARD_RESOURCE_ARR=(${TENSORBOARD_RESOURCE_ID//\\\\\\\r\n",
      "          // })\\n  PROJECT=${TENSORBOARD_RESOURCE_ARR[1]}\\n  LOCATION=${TENSORBOARD_RESOURCE_ARR[3]}\\n\\\r\n",
      "          \\  TENSORBOARD_ID=${TENSORBOARD_RESOURCE_ARR[5]}\\nelse\\n  echo '[ERROR]:\\\r\n",
      "          \\ Invalid format of tensorboard_resource_id. It must be a string with format\\\r\n",
      "          \\ projects/${PROJECT_NUMBER}/locations/${LOCATION}/tensorboards/${TENSORBOARD_ID}'\\n\\\r\n",
      "          \\  exit 1\\nfi\\n\\nset +e\\n\\n/opt/conda/bin/tb-gcp-uploader --tensorboard_resource_name\\\r\n",
      "          \\ \\\\\\n  \\\"${TENSORBOARD_RESOURCE_ID}\\\" \\\\\\n  --logdir=\\\"${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\ \\\\\\n  --experiment_name=\\\"${EXPERIMENT_NAME}\\\" \\\\\\n  --one_shot=True\\n\\\r\n",
      "          \\nif [ $? -ne 0 ]; then\\n  exit 13\\nfi\\n\\nset -e\\n\\nweb_server_uri=\\\"tensorboard.googleusercontent.com\\\"\\\r\n",
      "          \\ntensorboard_resource_name_uri=\\\"projects+${PROJECT}+locations+${LOCATION}+tensorboards+${TENSORBOARD_ID}+experiments+${EXPERIMENT_NAME}\\\"\\\r\n",
      "          \\necho -n \\\"https://${LOCATION}.${web_server_uri}/experiment/${tensorboard_resource_name_uri}\\\"\\\r\n",
      "          \\ > \\\"${TENSORBOARD_URI}\\\"\\n\"\r\n",
      "        image: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\r\n",
      "    exec-upload-tensorboard-metrics-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - '{{$.inputs.parameters[''tensorboard_resource_id'']}}'\r\n",
      "        - '{{$.inputs.artifacts[''metrics_directory''].path}}'\r\n",
      "        - '{{$.inputs.parameters[''experiment_name'']}}'\r\n",
      "        - '{{$.outputs.parameters[''tensorboard_uri''].output_file}}'\r\n",
      "        command:\r\n",
      "        - bash\r\n",
      "        - -c\r\n",
      "        - \"\\nset -e -x\\nTENSORBOARD_RESOURCE_ID=\\\"$0\\\"\\nMETRICS_DIRECTORY_URI=\\\"$1\\\"\\\r\n",
      "          \\nEXPERIMENT_NAME=\\\"$2\\\"\\nTENSORBOARD_URI=\\\"$3\\\"\\n\\nmkdir -p \\\"$(dirname\\\r\n",
      "          \\ ${TENSORBOARD_URI})\\\"\\nif [ -z \\\"${TENSORBOARD_RESOURCE_ID}\\\" ];\\nthen\\n\\\r\n",
      "          \\  echo \\\"TensorBoard ID is not set. Skip uploading the TensorBoard.\\\"\\n\\\r\n",
      "          \\  echo -n \\\"\\\" > \\\"${TENSORBOARD_URI}\\\"\\n  exit 0\\nfi\\n\\nif [ -z \\\"${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\ ]; then\\n  echo \\\"Metrics directory uri is not set.\\\"\\n  exit 1\\nelif\\\r\n",
      "          \\ [ -z \\\"${EXPERIMENT_NAME}\\\" ]; then\\n  echo \\\"Experiment name is not set.\\\"\\\r\n",
      "          \\n  exit 1\\nelif [ -z \\\"${TENSORBOARD_URI}\\\" ]; then\\n  echo \\\"TensorBoard\\\r\n",
      "          \\ URI is not set.\\\"\\n  exit 1\\nfi\\n\\ncase \\\"${METRICS_DIRECTORY_URI}\\\" in\\n\\\r\n",
      "          \\  \\\"gs://\\\"*) ;;\\n  \\\"/gcs/\\\"*)\\n    METRICS_DIRECTORY_URI=${METRICS_DIRECTORY_URI/\\\"\\\r\n",
      "          /gcs/\\\"/\\\"gs://\\\"}\\n    echo \\\"Replaced /gcs/ path with ${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\n    ;;\\n  *)\\n    echo \\\"Invalid metrics directory uri. Metrics directory\\\r\n",
      "          \\ uri must start with gs:// or /gcs/.\\\"\\n    exit 1\\n    ;;\\nesac\\n\\nif\\\r\n",
      "          \\ [[ \\\"${TENSORBOARD_RESOURCE_ID}\\\" =~ ^projects/[^/]+/locations/[^/]+/tensorboards/[0-9]+$\\\r\n",
      "          \\ ]]; then\\n  echo \\\"Split tensorboard resource id\\\"\\n  TENSORBOARD_RESOURCE_ARR=(${TENSORBOARD_RESOURCE_ID//\\\\\\\r\n",
      "          // })\\n  PROJECT=${TENSORBOARD_RESOURCE_ARR[1]}\\n  LOCATION=${TENSORBOARD_RESOURCE_ARR[3]}\\n\\\r\n",
      "          \\  TENSORBOARD_ID=${TENSORBOARD_RESOURCE_ARR[5]}\\nelse\\n  echo '[ERROR]:\\\r\n",
      "          \\ Invalid format of tensorboard_resource_id. It must be a string with format\\\r\n",
      "          \\ projects/${PROJECT_NUMBER}/locations/${LOCATION}/tensorboards/${TENSORBOARD_ID}'\\n\\\r\n",
      "          \\  exit 1\\nfi\\n\\nset +e\\n\\n/opt/conda/bin/tb-gcp-uploader --tensorboard_resource_name\\\r\n",
      "          \\ \\\\\\n  \\\"${TENSORBOARD_RESOURCE_ID}\\\" \\\\\\n  --logdir=\\\"${METRICS_DIRECTORY_URI}\\\"\\\r\n",
      "          \\ \\\\\\n  --experiment_name=\\\"${EXPERIMENT_NAME}\\\" \\\\\\n  --one_shot=True\\n\\\r\n",
      "          \\nif [ $? -ne 0 ]; then\\n  exit 13\\nfi\\n\\nset -e\\n\\nweb_server_uri=\\\"tensorboard.googleusercontent.com\\\"\\\r\n",
      "          \\ntensorboard_resource_name_uri=\\\"projects+${PROJECT}+locations+${LOCATION}+tensorboards+${TENSORBOARD_ID}+experiments+${EXPERIMENT_NAME}\\\"\\\r\n",
      "          \\necho -n \\\"https://${LOCATION}.${web_server_uri}/experiment/${tensorboard_resource_name_uri}\\\"\\\r\n",
      "          \\ > \\\"${TENSORBOARD_URI}\\\"\\n\"\r\n",
      "        image: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\r\n",
      "    exec-value-exists:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - value_exists\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef value_exists(value: Optional[str] = None) -> bool:\\n  \\\"\\\"\\\"\\\r\n",
      "          Returns whether a runtime parameter was provided.\\n\\n  Args:\\n    value:\\\r\n",
      "          \\ That might have been provided.\\n\\n  Returns:\\n    Whether the string is\\\r\n",
      "          \\ not None and non-empty.\\n  \\\"\\\"\\\"\\n  if not value:\\n    return False\\n\\\r\n",
      "          \\  return True\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "    exec-value-exists-2:\r\n",
      "      container:\r\n",
      "        args:\r\n",
      "        - --executor_input\r\n",
      "        - '{{$}}'\r\n",
      "        - --function_to_execute\r\n",
      "        - value_exists\r\n",
      "        command:\r\n",
      "        - sh\r\n",
      "        - -ec\r\n",
      "        - 'program_path=$(mktemp -d)\r\n",
      "\r\n",
      "\r\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\r\n",
      "\r\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\r\n",
      "\r\n",
      "          '\r\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\r\n",
      "          \\ *\\n\\ndef value_exists(value: Optional[str] = None) -> bool:\\n  \\\"\\\"\\\"\\\r\n",
      "          Returns whether a runtime parameter was provided.\\n\\n  Args:\\n    value:\\\r\n",
      "          \\ That might have been provided.\\n\\n  Returns:\\n    Whether the string is\\\r\n",
      "          \\ not None and non-empty.\\n  \\\"\\\"\\\"\\n  if not value:\\n    return False\\n\\\r\n",
      "          \\  return True\\n\\n\"\r\n",
      "        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.5.0\r\n",
      "pipelineInfo:\r\n",
      "  description: Performs reinforcement learning from human feedback.\r\n",
      "  name: rlhf-train-template\r\n",
      "root:\r\n",
      "  dag:\r\n",
      "    outputs:\r\n",
      "      parameters:\r\n",
      "        endpoint_resource_name:\r\n",
      "          valueFromParameter:\r\n",
      "            outputParameterKey: endpoint_resource_name\r\n",
      "            producerSubtask: create-endpoint-and-deploy-model\r\n",
      "        model_resource_name:\r\n",
      "          valueFromParameter:\r\n",
      "            outputParameterKey: model_resource_name\r\n",
      "            producerSubtask: upload-llm-model\r\n",
      "    tasks:\r\n",
      "      condition-1:\r\n",
      "        componentRef:\r\n",
      "          name: comp-condition-1\r\n",
      "        dependentTasks:\r\n",
      "        - rewardmodeltrainer\r\n",
      "        - value-exists\r\n",
      "        inputs:\r\n",
      "          artifacts:\r\n",
      "            pipelinechannel--rewardmodeltrainer-tensorboard_metrics:\r\n",
      "              taskOutputArtifact:\r\n",
      "                outputArtifactKey: tensorboard_metrics\r\n",
      "                producerTask: rewardmodeltrainer\r\n",
      "          parameters:\r\n",
      "            pipelinechannel--tensorboard_resource_id:\r\n",
      "              componentInputParameter: tensorboard_resource_id\r\n",
      "            pipelinechannel--value-exists-Output:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: value-exists\r\n",
      "        taskInfo:\r\n",
      "          name: Upload Reward Model Tensorboard Metrics\r\n",
      "        triggerPolicy:\r\n",
      "          condition: inputs.parameter_values['pipelinechannel--value-exists-Output']\r\n",
      "            == true\r\n",
      "      condition-2:\r\n",
      "        componentRef:\r\n",
      "          name: comp-condition-2\r\n",
      "        dependentTasks:\r\n",
      "        - reinforcer\r\n",
      "        - value-exists\r\n",
      "        inputs:\r\n",
      "          artifacts:\r\n",
      "            pipelinechannel--reinforcer-tensorboard_metrics:\r\n",
      "              taskOutputArtifact:\r\n",
      "                outputArtifactKey: tensorboard_metrics\r\n",
      "                producerTask: reinforcer\r\n",
      "          parameters:\r\n",
      "            pipelinechannel--tensorboard_resource_id:\r\n",
      "              componentInputParameter: tensorboard_resource_id\r\n",
      "            pipelinechannel--value-exists-Output:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: value-exists\r\n",
      "        taskInfo:\r\n",
      "          name: Upload Reinforcement Learning Tensorboard Metrics\r\n",
      "        triggerPolicy:\r\n",
      "          condition: inputs.parameter_values['pipelinechannel--value-exists-Output']\r\n",
      "            == true\r\n",
      "      condition-3:\r\n",
      "        componentRef:\r\n",
      "          name: comp-condition-3\r\n",
      "        dependentTasks:\r\n",
      "        - reinforcer\r\n",
      "        - value-exists-2\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            pipelinechannel--eval_dataset:\r\n",
      "              componentInputParameter: eval_dataset\r\n",
      "            pipelinechannel--instruction:\r\n",
      "              componentInputParameter: instruction\r\n",
      "            pipelinechannel--large_model_reference:\r\n",
      "              componentInputParameter: large_model_reference\r\n",
      "            pipelinechannel--location:\r\n",
      "              componentInputParameter: location\r\n",
      "            pipelinechannel--project:\r\n",
      "              componentInputParameter: project\r\n",
      "            pipelinechannel--prompt_sequence_length:\r\n",
      "              componentInputParameter: prompt_sequence_length\r\n",
      "            pipelinechannel--reinforcer-output_model_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: output_model_path\r\n",
      "                producerTask: reinforcer\r\n",
      "            pipelinechannel--target_sequence_length:\r\n",
      "              componentInputParameter: target_sequence_length\r\n",
      "            pipelinechannel--value-exists-2-Output:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: value-exists-2\r\n",
      "        taskInfo:\r\n",
      "          name: Perform Inference\r\n",
      "        triggerPolicy:\r\n",
      "          condition: inputs.parameter_values['pipelinechannel--value-exists-2-Output']\r\n",
      "            == true\r\n",
      "      convert-to-delimited-string:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-convert-to-delimited-string\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            items:\r\n",
      "              runtimeValue:\r\n",
      "                constant:\r\n",
      "                - candidate_0\r\n",
      "                - candidate_1\r\n",
      "        taskInfo:\r\n",
      "          name: convert-to-delimited-string\r\n",
      "      create-endpoint-and-deploy-model:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-create-endpoint-and-deploy-model\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-deploy-model\r\n",
      "        - resolve-model-display-name\r\n",
      "        - resolve-regional-endpoint\r\n",
      "        - upload-llm-model\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            deploy_model:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-deploy-model\r\n",
      "            display_name:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-model-display-name\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us-central1\r\n",
      "            model_resource_name:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: model_resource_name\r\n",
      "                producerTask: upload-llm-model\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: '{{$.pipeline_google_cloud_project_id}}'\r\n",
      "            regional_endpoint:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-regional-endpoint\r\n",
      "        taskInfo:\r\n",
      "          name: Deploy Model\r\n",
      "      importer:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-importer\r\n",
      "        dependentTasks:\r\n",
      "        - reinforcer\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            uri:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: output_adapter_path\r\n",
      "                producerTask: reinforcer\r\n",
      "        taskInfo:\r\n",
      "          name: Import Tuned Adapter\r\n",
      "      privatetextcomparisonimporter:\r\n",
      "        cachingOptions: {}\r\n",
      "        componentRef:\r\n",
      "          name: comp-privatetextcomparisonimporter\r\n",
      "        dependentTasks:\r\n",
      "        - convert-to-delimited-string\r\n",
      "        - resolve-image-uri-2\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            choice_field_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: choice\r\n",
      "            comma_separated_candidates_field_names:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: convert-to-delimited-string\r\n",
      "            image_uri:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-image-uri-2\r\n",
      "            input_text:\r\n",
      "              componentInputParameter: preference_dataset\r\n",
      "            inputs_field_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: input_text\r\n",
      "            instruction:\r\n",
      "              componentInputParameter: instruction\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: reward_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            location:\r\n",
      "              componentInputParameter: location\r\n",
      "            project:\r\n",
      "              componentInputParameter: project\r\n",
      "            split:\r\n",
      "              runtimeValue:\r\n",
      "                constant: train\r\n",
      "        taskInfo:\r\n",
      "          name: Import Preference Dataset\r\n",
      "      privatetextimporter:\r\n",
      "        cachingOptions: {}\r\n",
      "        componentRef:\r\n",
      "          name: comp-privatetextimporter\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-image-uri\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            image_uri:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-image-uri\r\n",
      "            input_text:\r\n",
      "              componentInputParameter: prompt_dataset\r\n",
      "            inputs_field_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: input_text\r\n",
      "            instruction:\r\n",
      "              componentInputParameter: instruction\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: large_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            location:\r\n",
      "              componentInputParameter: location\r\n",
      "            output_split_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: train\r\n",
      "            project:\r\n",
      "              componentInputParameter: project\r\n",
      "            targets_field_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: non_existent_targets_field_name\r\n",
      "        taskInfo:\r\n",
      "          name: Import Prompt Dataset\r\n",
      "      reinforcer:\r\n",
      "        cachingOptions: {}\r\n",
      "        componentRef:\r\n",
      "          name: comp-reinforcer\r\n",
      "        dependentTasks:\r\n",
      "        - privatetextimporter\r\n",
      "        - resolve-image-uri-4\r\n",
      "        - resolve-machine-spec\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        - rewardmodeltrainer\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            accelerator_count:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_count\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            accelerator_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            batch_size:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 64.0\r\n",
      "            image_uri:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-image-uri-4\r\n",
      "            input_dataset_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: imported_data_path\r\n",
      "                producerTask: privatetextimporter\r\n",
      "            input_reference_model_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: reference_model_path\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            input_reward_model_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: output_model_path\r\n",
      "                producerTask: rewardmodeltrainer\r\n",
      "            inputs_sequence_length:\r\n",
      "              componentInputParameter: prompt_sequence_length\r\n",
      "            kl_coeff:\r\n",
      "              componentInputParameter: kl_coeff\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: large_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            learning_rate_multiplier:\r\n",
      "              componentInputParameter: reinforcement_learning_rate_multiplier\r\n",
      "            location:\r\n",
      "              componentInputParameter: location\r\n",
      "            lora_dim:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 1.0\r\n",
      "            machine_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: machine_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            project:\r\n",
      "              componentInputParameter: project\r\n",
      "            reward_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: reward_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            targets_sequence_length:\r\n",
      "              componentInputParameter: target_sequence_length\r\n",
      "            train_steps:\r\n",
      "              componentInputParameter: reinforcement_learning_train_steps\r\n",
      "        taskInfo:\r\n",
      "          name: Reinforcer\r\n",
      "      resolve-deploy-model:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-deploy-model\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            deploy_model:\r\n",
      "              componentInputParameter: deploy_model\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: large_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Deploy Model\r\n",
      "      resolve-image-uri:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-image-uri\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            artifact_registry:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf\r\n",
      "            image_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: text_importer\r\n",
      "            image_name_prefix:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf_\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: vertex-ai-restricted\r\n",
      "            tag:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 20231010_1107_RC00\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Prompt Dataset Image URI\r\n",
      "      resolve-image-uri-2:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-image-uri-2\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            artifact_registry:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf\r\n",
      "            image_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: text_comparison_importer\r\n",
      "            image_name_prefix:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf_\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: vertex-ai-restricted\r\n",
      "            tag:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 20231010_1107_RC00\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Preference Dataset Image URI\r\n",
      "      resolve-image-uri-3:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-image-uri-3\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-machine-spec\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            accelerator_count:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_count\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            accelerator_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            artifact_registry:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf\r\n",
      "            image_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: reward_model\r\n",
      "            image_name_prefix:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf_\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: vertex-ai-restricted\r\n",
      "            tag:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 20231010_1107_RC00\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Reward Model Image URI\r\n",
      "      resolve-image-uri-4:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-image-uri-4\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-machine-spec\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            accelerator_count:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_count\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            accelerator_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            artifact_registry:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf\r\n",
      "            image_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: reinforcer\r\n",
      "            image_name_prefix:\r\n",
      "              runtimeValue:\r\n",
      "                constant: rlhf_\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: vertex-ai-restricted\r\n",
      "            tag:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 20231010_1107_RC00\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Reinforcer Image URI\r\n",
      "      resolve-machine-spec:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-machine-spec\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            location:\r\n",
      "              componentInputParameter: location\r\n",
      "            use_test_spec:\r\n",
      "              runtimeValue:\r\n",
      "                constant: false\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Machine Spec\r\n",
      "      resolve-model-display-name:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-model-display-name\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: large_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            model_display_name:\r\n",
      "              componentInputParameter: model_display_name\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Model Display Name\r\n",
      "      resolve-reference-model-metadata:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            large_model_reference:\r\n",
      "              componentInputParameter: large_model_reference\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Model Metadata\r\n",
      "      resolve-regional-endpoint:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-regional-endpoint\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            upload_location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us-central1\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Regional Endpoint\r\n",
      "      resolve-upload-model:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-resolve-upload-model\r\n",
      "        dependentTasks:\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: large_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Upload Model\r\n",
      "      rewardmodeltrainer:\r\n",
      "        cachingOptions: {}\r\n",
      "        componentRef:\r\n",
      "          name: comp-rewardmodeltrainer\r\n",
      "        dependentTasks:\r\n",
      "        - privatetextcomparisonimporter\r\n",
      "        - resolve-image-uri-3\r\n",
      "        - resolve-machine-spec\r\n",
      "        - resolve-reference-model-metadata\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            accelerator_count:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_count\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            accelerator_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: accelerator_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            batch_size:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 64.0\r\n",
      "            image_uri:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-image-uri-3\r\n",
      "            input_dataset_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: output_dataset_path\r\n",
      "                producerTask: privatetextcomparisonimporter\r\n",
      "            input_model_path:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: reward_model_path\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            inputs_sequence_length:\r\n",
      "              componentInputParameter: prompt_sequence_length\r\n",
      "            large_model_reference:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: reward_model_reference\r\n",
      "                producerTask: resolve-reference-model-metadata\r\n",
      "            learning_rate_multiplier:\r\n",
      "              componentInputParameter: reward_model_learning_rate_multiplier\r\n",
      "            location:\r\n",
      "              componentInputParameter: location\r\n",
      "            lora_dim:\r\n",
      "              runtimeValue:\r\n",
      "                constant: 0.0\r\n",
      "            machine_type:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: machine_type\r\n",
      "                producerTask: resolve-machine-spec\r\n",
      "            project:\r\n",
      "              componentInputParameter: project\r\n",
      "            targets_sequence_length:\r\n",
      "              componentInputParameter: target_sequence_length\r\n",
      "            train_steps:\r\n",
      "              componentInputParameter: reward_model_train_steps\r\n",
      "        taskInfo:\r\n",
      "          name: Reward Model Trainer\r\n",
      "      upload-llm-model:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-upload-llm-model\r\n",
      "        dependentTasks:\r\n",
      "        - importer\r\n",
      "        - resolve-model-display-name\r\n",
      "        - resolve-regional-endpoint\r\n",
      "        - resolve-upload-model\r\n",
      "        inputs:\r\n",
      "          artifacts:\r\n",
      "            artifact_uri:\r\n",
      "              taskOutputArtifact:\r\n",
      "                outputArtifactKey: artifact\r\n",
      "                producerTask: importer\r\n",
      "          parameters:\r\n",
      "            location:\r\n",
      "              runtimeValue:\r\n",
      "                constant: us-central1\r\n",
      "            model_display_name:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-model-display-name\r\n",
      "            model_reference_name:\r\n",
      "              runtimeValue:\r\n",
      "                constant: text-bison@001\r\n",
      "            project:\r\n",
      "              runtimeValue:\r\n",
      "                constant: '{{$.pipeline_google_cloud_project_id}}'\r\n",
      "            regional_endpoint:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-regional-endpoint\r\n",
      "            upload_model:\r\n",
      "              taskOutputParameter:\r\n",
      "                outputParameterKey: Output\r\n",
      "                producerTask: resolve-upload-model\r\n",
      "        taskInfo:\r\n",
      "          name: Upload Model\r\n",
      "      value-exists:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-value-exists\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            value:\r\n",
      "              componentInputParameter: tensorboard_resource_id\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Tensorboard Resource ID\r\n",
      "      value-exists-2:\r\n",
      "        cachingOptions:\r\n",
      "          enableCache: true\r\n",
      "        componentRef:\r\n",
      "          name: comp-value-exists-2\r\n",
      "        inputs:\r\n",
      "          parameters:\r\n",
      "            value:\r\n",
      "              componentInputParameter: eval_dataset\r\n",
      "        taskInfo:\r\n",
      "          name: Resolve Inference Dataset\r\n",
      "  inputDefinitions:\r\n",
      "    parameters:\r\n",
      "      deploy_model:\r\n",
      "        defaultValue: true\r\n",
      "        description: Whether to deploy the model to an endpoint in `us-central1`.\r\n",
      "          Default is True.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: BOOLEAN\r\n",
      "      eval_dataset:\r\n",
      "        description: Optional Cloud storage path to an evaluation dataset. If provided,\r\n",
      "          inference will be performed on this dataset after training. The dataset\r\n",
      "          format is jsonl. Each example in the dataset must contain a field `input_text`\r\n",
      "          that contains the prompt.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "      instruction:\r\n",
      "        description: This field lets the model know what task it needs to perform.\r\n",
      "          Base models have been trained over a large set of varied instructions. You\r\n",
      "          can give a simple and intuitive description of the task and the model will\r\n",
      "          follow it, e.g. \"Classify this movie review as positive or negative\" or\r\n",
      "          \"Translate this sentence to Danish\". Do not specify this if your dataset\r\n",
      "          already prepends the instruction to the inputs field.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "      kl_coeff:\r\n",
      "        defaultValue: 0.1\r\n",
      "        description: Coefficient for KL penalty. This regularizes the policy model\r\n",
      "          and penalizes if it diverges from its initial distribution. If set to 0,\r\n",
      "          the reference language model is not loaded into memory. Default value is\r\n",
      "          0.1.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_DOUBLE\r\n",
      "      large_model_reference:\r\n",
      "        description: Name of the base model. Supported values are `text-bison@001`,\r\n",
      "          `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\r\n",
      "          are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl` and\r\n",
      "          `t5-xxl` are only supported in `europe-west4`.\r\n",
      "        parameterType: STRING\r\n",
      "      location:\r\n",
      "        defaultValue: '{{$.pipeline_google_cloud_location}}'\r\n",
      "        description: Location used to run custom jobs. If not specified the location\r\n",
      "          used to run the pipeline will be used.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "      model_display_name:\r\n",
      "        description: Name of the fine-tuned model shown in the Model Registry. If\r\n",
      "          not provided, a default name will be created.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "      preference_dataset:\r\n",
      "        description: 'Cloud storage path to a human preference dataset used to train\r\n",
      "          a reward model. The dataset format is jsonl. Each example in the dataset\r\n",
      "          must contain the following fields: `input_text` that contains the prompt,\r\n",
      "          `candidate_0` and `candidate_1` that contain candidate responses, `choice`\r\n",
      "          that specifies the preferred candidate.'\r\n",
      "        parameterType: STRING\r\n",
      "      project:\r\n",
      "        defaultValue: '{{$.pipeline_google_cloud_project_id}}'\r\n",
      "        description: Project used to run custom jobs. If not specified the project\r\n",
      "          used to run the pipeline will be used.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "      prompt_dataset:\r\n",
      "        description: Cloud storage path to an unlabled prompt dataset used for reinforcement\r\n",
      "          learning. The dataset format is jsonl. Each example in the dataset must\r\n",
      "          have an `input_text` field that contains the prompt.\r\n",
      "        parameterType: STRING\r\n",
      "      prompt_sequence_length:\r\n",
      "        defaultValue: 512.0\r\n",
      "        description: Maximum tokenized sequence length for input text. Higher values\r\n",
      "          increase memory overhead. This value should be at most 8192. Default value\r\n",
      "          is 512.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_INTEGER\r\n",
      "      reinforcement_learning_rate_multiplier:\r\n",
      "        defaultValue: 1.0\r\n",
      "        description: Constant used to adjust the base learning rate used during reinforcement\r\n",
      "          learning. Multiply by a number > 1 to increase the magnitude of updates\r\n",
      "          applied at each training step or multiply by a number < 1 to decrease the\r\n",
      "          magnitude of updates. Default value is 1.0.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_DOUBLE\r\n",
      "      reinforcement_learning_train_steps:\r\n",
      "        defaultValue: 1000.0\r\n",
      "        description: Number of reinforcement learning steps to perform when tuning\r\n",
      "          a base model. Default value is 1000.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_INTEGER\r\n",
      "      reward_model_learning_rate_multiplier:\r\n",
      "        defaultValue: 1.0\r\n",
      "        description: Constant used to adjust the base learning rate used when training\r\n",
      "          a reward model. Multiply by a number > 1 to increase the magnitude of updates\r\n",
      "          applied at each training step or multiply by a number < 1 to decrease the\r\n",
      "          magnitude of updates. Default value is 1.0.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_DOUBLE\r\n",
      "      reward_model_train_steps:\r\n",
      "        defaultValue: 1000.0\r\n",
      "        description: Number of steps to use when training a reward model. Default\r\n",
      "          value is 1000.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_INTEGER\r\n",
      "      target_sequence_length:\r\n",
      "        defaultValue: 64.0\r\n",
      "        description: ' Maximum tokenized sequence length for target text. Higher values\r\n",
      "          increase memory overhead. This value should be at most 1024. Default value\r\n",
      "          is 64.'\r\n",
      "        isOptional: true\r\n",
      "        parameterType: NUMBER_INTEGER\r\n",
      "      tensorboard_resource_id:\r\n",
      "        description: Optional tensorboard resource id in format `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\r\n",
      "          If provided, tensorboard metrics will be uploaded to this location.\r\n",
      "        isOptional: true\r\n",
      "        parameterType: STRING\r\n",
      "  outputDefinitions:\r\n",
      "    parameters:\r\n",
      "      endpoint_resource_name:\r\n",
      "        description: Path the Online Prediction Endpoint. This will be an empty string\r\n",
      "          if the model was not deployed.\r\n",
      "        parameterType: STRING\r\n",
      "      model_resource_name:\r\n",
      "        description: Path to the model uploaded to the Model Registry. This will be\r\n",
      "          an empty string if the model was not deployed.\r\n",
      "        parameterType: STRING\r\n",
      "schemaVersion: 2.1.0\r\n",
      "sdkVersion: kfp-2.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat rlhf_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14851c",
   "metadata": {},
   "source": [
    "## Define the Vertex AI pipeline job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe574a96",
   "metadata": {},
   "source": [
    "### Define the location of the training and evaluation data\n",
    "Previously, the datasets were loaded from small JSONL files, but for typical training jobs, the datasets are much larger, and are usually stored in cloud storage (in this case, Google Cloud Storage).\n",
    "\n",
    "**Note:** Make sure that the three datasets are stored in the same Google Cloud Storage bucket.\n",
    "```Python\n",
    "parameter_values={\n",
    "        \"preference_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n",
    "        \"prompt_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl\",\n",
    "        \"eval_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl\",\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6410f32-475f-4fc6-a751-5fe627312085",
   "metadata": {},
   "source": [
    "### Choose the foundation model to be tuned\n",
    "\n",
    "In this case, we are tuning the [Llama-2](https://ai.meta.com/llama/) foundational model, the LLM to tune is called **large_model_reference**. \n",
    "\n",
    "In this course, we're tuning the llama-2-7b, but you can also run an RLHF pipeline on Vertex AI to tune models such as: the T5x or text-bison@001. \n",
    "\n",
    "```Python\n",
    "parameter_values={\n",
    "        \"large_model_reference\": \"llama-2-7b\",\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff91eb",
   "metadata": {},
   "source": [
    "### Calculate the number of reward model training steps\n",
    "\n",
    "**reward_model_train_steps** is the number of steps to use when training the reward model.  This depends on the size of your preference dataset. We recommend the model should train over the preference dataset for 20-30 epochs for best results.\n",
    "\n",
    "$$ stepsPerEpoch = \\left\\lceil \\frac{datasetSize}{batchSize} \\right\\rceil$$\n",
    "$$ trainSteps = stepsPerEpoch \\times numEpochs$$\n",
    "\n",
    "The RLHF pipeline parameters are asking for the number of training steps and not number of epochs. Here's an example of how to go from epochs to training steps, given that the batch size for this pipeline is fixed at 64 examples per batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1031a7d-ed33-451b-aac4-1e1b616826c4",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Preference dataset size\n",
    "PREF_DATASET_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cbe0db-19de-4b0b-bfd7-8bd06a22defa",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Batch size is fixed at 64\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c94da3-3016-4743-baeb-50bedd330328",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfb58a8-9498-41b3-afc8-7ef81c4a7404",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "REWARD_STEPS_PER_EPOCH = math.ceil(PREF_DATASET_SIZE / BATCH_SIZE)\n",
    "print(REWARD_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe291d6-6d5c-4fb8-a783-61fb21269766",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "REWARD_NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83ffdc9-29d6-40e7-be30-06693816de63",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Calculate number of steps in the reward model training\n",
    "reward_model_train_steps = REWARD_STEPS_PER_EPOCH * REWARD_NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f3df46-868f-470f-b4db-bbcd4ca6dfd2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n"
     ]
    }
   ],
   "source": [
    "print(reward_model_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e866f-828e-4d5b-9d42-d206b57cb0b9",
   "metadata": {},
   "source": [
    "### Calculate the number of reinforcement learning training steps\n",
    "The **reinforcement_learning_train_steps** parameter is the number of reinforcement learning steps to perform when tuning the base model. \n",
    "- The number of training steps depends on the size of your prompt dataset. Usually, this model should train over the prompt dataset for roughly 10-20 epochs.\n",
    "- Reward hacking: if given too many training steps, the policy model may figure out a way to exploit the reward and exhibit undesired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6880ae3-8977-4605-9b8d-e50013c03896",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Prompt dataset size\n",
    "PROMPT_DATASET_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e1f705-9ff2-4204-b4f9-96bcaacf798c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Batch size is fixed at 64\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898fd671-e3f1-4174-8c63-dd64af6baa1d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30281c30-3b29-4ec0-b7fc-0df4f0203349",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "RL_STEPS_PER_EPOCH = math.ceil(PROMPT_DATASET_SIZE / BATCH_SIZE)\n",
    "print(RL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3415a4c5-c816-46e1-8b1e-2b6b976f2653",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "RL_NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d16b09f-5e48-4c2e-bb33-0b719e0943fa",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Calculate the number of steps in the RL training\n",
    "reinforcement_learning_train_steps = RL_STEPS_PER_EPOCH * RL_NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4079f9-0816-46c3-937f-3c85a491eb22",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "print(reinforcement_learning_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b95cf-9f6f-45c2-810f-363f761a235b",
   "metadata": {},
   "source": [
    "### Define the instruction\n",
    "\n",
    "- Choose the task-specific instruction that you want to use to tune the foundational model.  For this example, the instruction is \"Summarize in less than 50 words.\"\n",
    "- You can choose different instructions, for example, \"Write a reply to the following question or comment.\" Note that you would also need to collect your preference dataset with the same instruction added to the prompt, so that both the responses and the human preferences are based on that instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbd66c-aeb2-4bf8-97f0-eba82e5de51e",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Completed values for the dictionary\n",
    "parameter_values={\n",
    "        \"preference_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n",
    "        \"prompt_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl\",\n",
    "        \"eval_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl\",\n",
    "        \"large_model_reference\": \"llama-2-7b\",\n",
    "        \"reward_model_train_steps\": 1410,\n",
    "        \"reinforcement_learning_train_steps\": 320, # results from the calculations above\n",
    "        \"reward_model_learning_rate_multiplier\": 1.0,\n",
    "        \"reinforcement_learning_rate_multiplier\": 1.0,\n",
    "        \"kl_coeff\": 0.1, # increased to reduce reward hacking\n",
    "        \"instruction\":\\\n",
    "    \"Summarize in less than 50 words\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c88a7",
   "metadata": {},
   "source": [
    "### Train with full dataset: dictionary 'parameter_values' \n",
    "\n",
    "- Adjust the settings for training with the full dataset to achieve optimal results in the evaluation (next lesson). Take a look at the new values; these results are from various training experiments in the pipeline, and the best parameter values are displayed here.\n",
    "\n",
    "```python\n",
    "parameter_values={\n",
    "        \"preference_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n",
    "        \"prompt_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text/reddit_tfds/train/*.jsonl\",\n",
    "        \"eval_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text/reddit_tfds/val/*.jsonl\",\n",
    "        \"large_model_reference\": \"llama-2-7b\",\n",
    "        \"reward_model_train_steps\": 10000,\n",
    "        \"reinforcement_learning_train_steps\": 10000, \n",
    "        \"reward_model_learning_rate_multiplier\": 1.0,\n",
    "        \"reinforcement_learning_rate_multiplier\": 0.2,\n",
    "        \"kl_coeff\": 0.1,\n",
    "        \"instruction\":\\\n",
    "    \"Summarize in less than 50 words\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc6d51",
   "metadata": {},
   "source": [
    "### Set up Google Cloud to run the Vertex AI pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9571014",
   "metadata": {},
   "source": [
    "Vertex AI is already installed in this classroom environment.  If you were running this on your own project, you would install Vertex AI SDK like this:\n",
    "```Python\n",
    "!pip3 install google-cloud-aiplatform\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820b9b6-d93c-492c-8e0f-7570d4bc67c1",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Authenticate in utils\n",
    "from utils import authenticate\n",
    "credentials, PROJECT_ID, STAGING_BUCKET = authenticate()\n",
    "\n",
    "# RLFH pipeline is available in this region\n",
    "REGION = \"europe-west4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf92aac",
   "metadata": {},
   "source": [
    "## Run the pipeline job on Vertex AI\n",
    "\n",
    "Now that we have created our dictionary of values, we can create a PipelineJob. This just means that the RLHF pipeline will execute on Vertex AI. So it's not running locally here in the notebook, but on some server on Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1642b4-d16d-4e9b-ba87-3391168c5a11",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427dc778-9a04-4816-8569-0b5ea1c39f14",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID,\n",
    "                location = REGION,\n",
    "                credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eac3e3-2d17-47d7-b69f-97a20e91042b",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Look at the path for the YAML file\n",
    "RLHF_PIPELINE_PKG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa58dce",
   "metadata": {},
   "source": [
    "### Create and run the pipeline job\n",
    "- Here is how you would create the pipeline job and run it if you were working on your own project.\n",
    "- This job takes about a full day to run with multiple accelerators (TPUs/GPUs), and so we're not going to run it in this classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3cf10",
   "metadata": {},
   "source": [
    "- To create the pipeline job:\n",
    "\n",
    "```Python\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"tutorial-rlhf-tuning\",\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    template_path=RLHF_PIPELINE_PKG_PATH,\n",
    "    parameter_values=parameter_values)\n",
    "```\n",
    "- To run the pipeline job:\n",
    "\n",
    "```Python\n",
    "job.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bec5c2-7bc7-48f5-93e8-9bb9e5486000",
   "metadata": {},
   "source": [
    "- The content team has run this RLHF training pipeline to tune the Llama-2 model, and in the next lesson, you'll get to evaluate the log data to compare the performance of the tuned model with the original foundational model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
