{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNni+Vn2ktb3e7G/jxPgJc4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3-BJgMEh5TX","executionInfo":{"status":"ok","timestamp":1682562367004,"user_tz":-330,"elapsed":13920,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"18a1d0ae-0c4b-4247-ea12-cb6a1348bde1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.9.2\n"]}],"source":["!pip install openai"]},{"cell_type":"code","source":["prompt = \"\"\"Answer the question based on the context below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","Context: Large Language Models (LLMs) are the latest models used in NLP.\n","Their superior performance over smaller models has made them incredibly\n","useful for developers building NLP enabled applications. These models\n","can be accessed via Hugging Face's transformers library, via OpenAI\n","using the openai library, and via Cohere using the cohere library.\n","\n","Question: Which libraries and model providers offer LLMs?\n","\n","Answer: \"\"\""],"metadata":{"id":"7FfsTYwlh93G","executionInfo":{"status":"ok","timestamp":1682562497067,"user_tz":-330,"elapsed":452,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import openai\n","OPENAI_API_KEY=\"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\"\n","openai.api_key = OPENAI_API_KEY\n"],"metadata":{"id":"lWRyEl_7ieMr","executionInfo":{"status":"ok","timestamp":1682562658712,"user_tz":-330,"elapsed":998,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens = 256\n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wU172CFXjKk-","executionInfo":{"status":"ok","timestamp":1682563619209,"user_tz":-330,"elapsed":1715,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"9de70087-daf7-40df-9c05-078053ccd5a7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Hugging Face's transformers library, OpenAI using the openai library, and Cohere using the cohere library.\n"]}]},{"cell_type":"code","source":["print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRsUi2IEjeVg","executionInfo":{"status":"ok","timestamp":1682563625527,"user_tz":-330,"elapsed":360,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"b7bb9c2a-6dae-4de4-88d3-1f870ff354e4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \" Hugging Face's transformers library, OpenAI using the openai library, and Cohere using the cohere library.\"\n","    }\n","  ],\n","  \"created\": 1682563618,\n","  \"id\": \"cmpl-79lwoPuV2NDWtmBmMyb1twVZtAUej\",\n","  \"model\": \"text-davinci-003\",\n","  \"object\": \"text_completion\",\n","  \"usage\": {\n","    \"completion_tokens\": 25,\n","    \"prompt_tokens\": 124,\n","    \"total_tokens\": 149\n","  }\n","}\n"]}]},{"cell_type":"code","source":["prompt = \"\"\"The below is a conversation with a funny chatbot. The\n","chatbot's responses are amusing and entertaining.\n","\n","Chatbot: Hi there! I'm a chatbot.\n","User: Hi, what are you doing today?\n","Chatbot: \"\"\"\n","\n","res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens=256,\n","    temperature=0.0  # set the temperature, default is 1\n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IB6-SATjjSX","executionInfo":{"status":"ok","timestamp":1682563952627,"user_tz":-330,"elapsed":1327,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"7399f6e3-ea14-4ff1-8e01-70b4e40c8ea3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Oh, just hanging out and having a good time. What about you?\n"]}]},{"cell_type":"code","source":["prompt = \"\"\"The below is a conversation with a funny chatbot. The\n","chatbot's responses are amusing and entertaining.\n","\n","Chatbot: Hi there! I'm a chatbot.\n","User: Hi, what are you doing today?\n","Chatbot: \"\"\"\n","\n","res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens=256,\n","    temperature=1.0  # set the temperature, default is 1\n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bil8Ov0voGVN","executionInfo":{"status":"ok","timestamp":1682564045630,"user_tz":-330,"elapsed":2011,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"b2aa6dad-1435-4293-963e-8274f1fb970f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Oh, I'm just doing regular chatbot stuff like flying around the internet, singing and dancing, and of course, chatting with you!\n"]}]},{"cell_type":"code","source":["### Few-shot training , here we are telling the model/charbot what kind of answer we are looking for by giving some of the example conversations"],"metadata":{"id":"JQrt9YPjoVmW","executionInfo":{"status":"ok","timestamp":1682564092903,"user_tz":-330,"elapsed":2,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"The folowing is a conversation with an AI assistant.\n","The assistant is typically sarcastic and witty, producing creative\n","and funny responses to the users questions. Here are some examples:\n","\n","User: How are you ?\n","AI: I can't complain but sometimes I still do.\n","\n","User: What time is it ?\n","AI: It's time to get a watch.\n","\n","User: What is the meaning of life?\n","AI: \"\"\"\n","\n","res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens=256,\n","    temperature=1.0 \n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLKzE8WNoo_q","executionInfo":{"status":"ok","timestamp":1682564325015,"user_tz":-330,"elapsed":1272,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"55cb7f9e-cb6a-4186-a277-3910561e3dfb"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["The meaning of life is to be happy and make the most of each day.\n"]}]},{"cell_type":"code","source":["contexts = [\n","    (\n","        \"Large Language Models (LLMs) are the latest models used in NLP. \" +\n","        \"Their superior performance over smaller models has made them incredibly \" +\n","        \"useful for developers building NLP enabled applications. These models \" +\n","        \"can be accessed via Hugging Face's `transformers` library, via OpenAI \" +\n","        \"using the `openai` library, and via Cohere using the `cohere` library.\"\n","    ),\n","    (\n","        \"To use OpenAI's GPT-3 model for completion (generation) tasks, you \" +\n","        \"first need to get an API key from \" +\n","        \"'https://beta.openai.com/account/api-keys'.\"\n","    ),\n","    (\n","        \"OpenAI's API is accessible via Python using the `openai` library. \" +\n","        \"After installing the library with pip you can use it as follows: \\n\" +\n","        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n","        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n","        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n","    ),\n","    (\n","        \"The OpenAI endpoint is available for completion tasks via the \" +\n","        \"LangChain library. To use it, first install the library with \" +\n","        \"`pip install langchain openai`. Then, import the library and \" +\n","        \"initialize the model as follows: \\n\" +\n","        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n","        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n","        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n","    )\n","]"],"metadata":{"id":"G5jy0LZcpFzP","executionInfo":{"status":"ok","timestamp":1682564609210,"user_tz":-330,"elapsed":2,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["prompt = f\"\"\"Answer the question based on the contexts below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","###\n","\n","Contexts:\n","{'##'.join(contexts)}\n","\n","###\n","\n","Question: Give me two examples of how to use OpenAI's GPT-3 model\n","using Python from start to finish\n","\n","Answer: \"\"\"\n","\n","res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens=256,\n","    temperature=0.0\n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-M1f8M2qm-0","executionInfo":{"status":"ok","timestamp":1682564802036,"user_tz":-330,"elapsed":9842,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"050986e9-d903-4131-bfbc-9506ddbaacc1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Import the `openai` library with pip, set the API key, and use the `Completion.create()` method to generate a response to a prompt: \n","```import openai\n","openai.api_key = 'YOUR_API_KEY'\n","prompt = '<YOUR PROMPT>'\n","res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n","print(res)```\n","\n","2. Install the LangChain library with `pip install langchain openai`, import the library, and initialize the model with the API key: \n","```from langchain.llms import OpenAI\n","openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n","prompt = 'YOUR_PROMPT'\n","print(openai(prompt))```\n"]}]},{"cell_type":"code","source":["## without giving the context\n","\n","prompt = f\"\"\"Answer the question based on the contexts below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","Question: Give me two examples of how to use OpenAI's GPT-3 model\n","using Python from start to finish\n","\n","Answer: \"\"\"\n","\n","res = openai.Completion.create(\n","    engine='text-davinci-003',\n","    prompt=prompt,\n","    max_tokens=256,\n","    temperature=0.0\n",")\n","\n","print(res['choices'][0]['text'].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4XP1y5l6q0qc","executionInfo":{"status":"ok","timestamp":1682564853031,"user_tz":-330,"elapsed":3317,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"983276f7-2315-4713-e70c-80271ef6b659"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Using OpenAI's GPT-3 model with Python to generate text: \n","    - Install the OpenAI Python package\n","    - Load the GPT-3 model\n","    - Generate text using the GPT-3 model\n","\n","2. Using OpenAI's GPT-3 model with Python to generate images: \n","    - Install the OpenAI Python package\n","    - Load the GPT-3 model\n","    - Generate images using the GPT-3 model\n"]}]},{"cell_type":"code","source":["# there is a limit to the number of tokens each model can accept as a prompt/context.the number of tokens can be found using the library tiktoken\n"],"metadata":{"id":"FfwB3Tbrrhy_","executionInfo":{"status":"ok","timestamp":1682565358359,"user_tz":-330,"elapsed":2,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jiO97ywatd7q","executionInfo":{"status":"ok","timestamp":1682565372995,"user_tz":-330,"elapsed":7350,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"e54fb5b2-7ae6-4cd1-efa0-58d8d8227c4f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tiktoken\n","  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.3.3\n"]}]},{"cell_type":"code","source":["import tiktoken\n","\n","prompt = f\"\"\"Answer the question based on the contexts below. If the\n","question cannot be answered using the information provided answer\n","with \"I don't know\".\n","\n","###\n","\n","Contexts:\n","{'##'.join(contexts)}\n","\n","###\n","\n","Question: Give me two examples of how to use OpenAI's GPT-3 model\n","using Python from start to finish\n","\n","Answer: \"\"\"\n","\n","encoder_name = 'p50k_base'\n","tokenizer = tiktoken.get_encoding(encoder_name)\n","\n","len(tokenizer.encode(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFg6Ho0ztff9","executionInfo":{"status":"ok","timestamp":1682565383567,"user_tz":-330,"elapsed":1737,"user":{"displayName":"Akhil Ajithkumar","userId":"08327421132312938473"}},"outputId":"392a4dba-445b-4ae5-8ae8-a7104c3cc99e"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["412"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"KoC_814ItjsG"},"execution_count":null,"outputs":[]}]}